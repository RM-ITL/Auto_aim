## 小陀螺迁移日志
# !!!!一定要检查相机线与USB口，相机线的受损和IO口的传输速率不够会影响相机的数据传输





# CMake框架迁移尝试：（迁移尝试搁置，受限于CMake框架的通讯与共享对象的管理过于原始化，想要达到Ros的标准考量花费时间以及精力过长，暂时搁置）
    问题：
1.高速移动相机，造成调试显示存在卡帧严重
2.代码过于臃肿，需要优化不必要的调试部分

# 5月27日：
1. 已测试分类功能，效果良好可以正常运行
2. 老问题，内存问题报错段错误——>段报错已解决：解决camera_catch中的ImageBufferPool中的空删除器，Mat对象未被正常删除，内存泄漏
3. 为Armorclassifer分类器采取新策略，避免每次预处理创建新的Tensor，新增Tensor的池化机制，管理Tensor池
4. 问题：快速移动相机有严重卡帧
  分析：imshow在主线程是同步阻塞的，高帧率会导致积累造成的延迟
  解决：新增异步显示的QuickDisplayFix的类，去替换inshow，实现异步显示，有所改善，但是问题仍然存在！！！
5. 测试：针对高速移动卡帧严重问题，非可视化与性能监控等调试问题带来的，常规纯推理也会带来高速移动卡帧的问题

# 5月31日
1. 问题： 调用QuickDisplayFix进行显示图像的时候，未推理时情况较好，但是推理时又会出现延迟


# Ros2框架迁移尝试

# 6月2日：
1. 优化原有代码：优化相机取流的camera_catch程序
   优化方案：
    1. 内存池技术 🏊‍♀️

    FrameBuffer类: 预分配32字节对齐的缓冲区
    MemoryPool类: 管理48个复用缓冲区（可配置）
    零运行时分配: 完全消除处理过程中的内存分配开销
    内存预热: 提前访问内存页，避免缺页异常

    2. 零拷贝数据流 🚀

    FrameData结构: 使用shared_ptr共享缓冲区，避免完整数据拷贝
    直接操作: 相机数据直接写入缓冲区，无需中间拷贝
    智能生命周期: 自动管理缓冲区释放，防止内存泄漏

    3. 无锁并发队列 ⚡

    LockFreeQueue: 基于原子操作的环形队列，容量256帧
    零锁竞争: 完全消除mutex/condition_variable开销
    高吞吐量: 支持高频率的生产者-消费者操作

    4. 实时性能监控 📊

    PerformanceMonitor: 精确统计帧率、延迟、丢帧率
    多维度监控: 采集/处理/发布各环节独立统计
    自动报告: 每5秒输出详细性能数据
    
    优化效果明显：
    [INFO] [1748843502.324770147] [camera_node]: === 相机性能统计 ===
    采集帧率: 57.34 fps
    处理帧率: 57.34 fps
    发布帧率: 57.34 fps
    丢帧率: 0.00%
    平均延迟: 6.48 ms
    最大延迟: 11.38 ms
    总采集帧数: 1980
    总处理帧数: 1980
    总丢弃帧数: 0


2. 优化armor_detector:
   📋 核心优化特性
    1. 异步推理管道

    请求池机制: 预创建4个ov::InferRequest对象，循环复用
    回调驱动: 使用OpenVINO 2024.6的set_callback()实现真正的异步处理
    内存预分配: 预分配tensor缓冲区，避免运行时内存分配

    2. 并发处理架构

    非阻塞回调: ROS图像回调只负责提交推理任务，立即返回
    专用处理线程: 独立线程处理推理结果，避免阻塞图像接收
    智能队列管理: 自动管理可用请求ID，支持并发访问

    3. 内存优化

    零拷贝预处理: 直接写入预分配tensor，减少数据拷贝
    优化的数据转换: BGR→RGB+归一化+NCHW转换一步完成
    智能内存复用: tensor缓冲区在请求间复用

    4. 帧管理机制

    帧跳过策略: 队列满时自动跳过新帧，防止内存爆炸
    性能监控: 实时统计处理率、跳帧率、队列长度等指标

    优化效果：
    [INFO] [1748848761.349230167] [yolo_subscriber]: 推理统计:
    [INFO] [1748848761.349234850] [yolo_subscriber]:   总推理次数: 1356
    [INFO] [1748848761.349239502] [yolo_subscriber]:   成功推理: 1356 (100.0%)
    [INFO] [1748848761.349245162] [yolo_subscriber]:   失败推理: 0
    [INFO] [1748848761.349249813] [yolo_subscriber]:   总检测数: 1088
    [INFO] [1748848761.349254373] [yolo_subscriber]:   平均检测率: 0.80 个/帧
    [INFO] [1748848761.349260692] [yolo_subscriber]: 性能指标:
    [INFO] [1748848761.349264956] [yolo_subscriber]:   平均推理时间: 19.61 ms
    [INFO] [1748848761.349270942] [yolo_subscriber]:   整体FPS: 54.24
    [INFO] [1748848761.349276383] [yolo_subscriber]:   运行时间: 25 秒
    [INFO] [1748848761.349282377] [yolo_subscriber]:   实时FPS: 60.00
    [INFO] [1748848761.349287605] [yolo_subscriber]:   内存使用: 18.75 MB
    [INFO] [1748848761.349293208] [yolo_subscriber]:   队列状态: 0/4 使用中


# 6月4日：
1. 消息数据迁移工作： 
  # 带测试——采取IDL语言定义自定义消息
  [方案]：
  简单消息直接转换为自定义消息[DetectResult等]，复杂消息转化为简化自定义加特化扩展和转化的接口恢复原有的功能[ArmorInfo等]
  ![alt text](image.png)

# 6月6日：
1. 自定义消息重新整理，合并角点信息和检测数据，塞数字分类到自定义消息里面，完善自定义消息数据
2. 做检测后过滤：
   # 1.原始模型基础过滤，装甲板和数字检测过滤
   # 2.建立检测目标优先级：Rank： 步兵>哨兵>英雄 ：联盟赛   
   # Rank: 英雄>步兵>哨兵>工程>前哨站>基地  (超对步兵)
   # Rank: 基地>前哨站>英雄>步兵>哨兵>工程 （超对英雄）
   过滤示例：
    红1目标A（置信度0.8）
    红1目标B（置信度0.6）
    红3目标C（置信度0.7）
    红3目标D（置信度0.5）
  应用联盟赛的优先级判断，过滤排序之后的就是 A>>A>B
  # 3.重叠过滤：针对相同目标进行重叠过滤，如果两个目标的区域存在重叠部分直接过滤，否则保留
  # 4.距离中心过滤，针对以上过滤之后的目标判断与画面中心的距离，如果超出阈值，直接剔除

  [整理]：
  假设检测到：红1_A, 红1_B, 红3_C, 红3_D

  全局过滤：都通过0.3阈值
  类别置信度：都通过0.5阈值
  特定阈值：都通过各自类别要求
  排序结果：红1_A → 红1_B → 红3_C → 红3_D（英雄rank=4优于步兵rank=6）
  NMS过滤：如果红1_A和红1_B重叠，保留红1_A
  最终输出：红1_A, 红3_C, 红3_D
  距离排序：按到中心距离重新排列最终显示/处理顺序

  已集成（多层过滤器）：
  [INFO] [1749213910.796945511] [simplified_armor_detector]: 多层过滤后检测到 2 个目标
  [INFO] [1749213910.797023861] [simplified_armor_detector]: 目标0: red:2 检测置信度:0.604 分类置信度:5.982 复合得分:3.613 优先级:未知(999) 中心距离:324.3
  [INFO] [1749213910.797063674] [simplified_armor_detector]: 目标1: red:4 检测置信度:0.594 分类置信度:7.751 复合得分:4.606 优先级:步兵(0) 中心距离:474.1
  [INFO] [1749213910.836710480] [simplified_armor_detector]: 多层过滤后检测到 2 个目标
  [INFO] [1749213910.836832762] [simplified_armor_detector]: 目标0: red:2 检测置信度:0.610 分类置信度:7.391 复合得分:4.510 优先级:未知(999) 中心距离:324.3
  [INFO] [1749213910.836859704] [simplified_armor_detector]: 目标1: red:4 检测置信度:0.601 分类置信度:6.833 复合得分:4.103 优先级:步兵(0) 中心距离:476.9
  [INFO] [1749213910.854540015] [simplified_armor_detector]: 多层过滤后检测到 2 个目标
  [INFO] [1749213910.854655495] [simplified_armor_detector]: 目标0: red:2 检测置信度:0.602 分类置信度:5.060 复合得分:3.048 优先级:未知(999) 中心距离:324.3
  [INFO] [1749213910.854700087] [simplified_armor_detector]: 目标1: red:4 检测置信度:0.585 分类置信度:7.873 复合得分:4.608 优先级:步兵(0) 中心距离:474.1

  [待迁移]：原有的装甲板熄灭处理和多车辆种类支持，同时现有程序待优化，简化现有代码，整理为自定义消息发布出去


3. 重构整理框架文件结构：
  下辖五个重要文件夹：
  <core>:核心的主节点（包含detected,predicted,serial等主节点）
  <algorithms>:包含为预测封装的所有程序与一些其他程序
  <config>:配置文件yaml存放热启动开关和各种预设参数
  <launch>：一键启动的launch文件
  <Messages>：自定义消息接口

# 6月7日
1. 搭建与反小陀螺对齐的消息数据：
  <Armor>：
  std_msgs/Header header
  geometry_msgs/Point[4] corners  # 0:左上，1：右上, 2：右下，3:左下
  string color
  float32 conf_color         # 置信度得分
  int32 number               # 分类数字
  float32 conf_number        # 数字分类置信度

  <Armor_result>:
  sensor_msgs/Image image
  # 四元数（表示方向）
  geometry_msgs/Quaternion orientation
  # 时间戳
  builtin_interfaces/Time timestamp
  # 装甲板信息
  Armor[] armor

2. 缩减代码行数，成功重构发布消息，现在发布的Armorresult消息类型

# 6月8日
1. 重构达[喵]陀螺仪的ROS2驱动程序例程，将官方的ROS1例程迁移到ROS2上，加入到当前框架进行测试：
    当前文件目录：
    src
    ├── [config]
    │   └── imu_params.yaml
    ├── [include]
    │   ├── bsp_crc.h
    │   └── imu_driver.h
    ├── [launch]
    │   ├── dm_rviz.launch.py
    │   ├── imu_launch.py
    │   └── imu_with_params.launch.py
    ├── [rviz]
    │   └── imu.rviz
    ├── [src]
    │   ├── bsp_crc.cpp
    │   ├── imu_driver.cpp
    │   └── test_imu.cpp
    ├── CMakeLists.txt
    └── package.xml

    <Result>:已成功实现DM—IMU的Ros2驱动迁移，当前发布频率为185hz，待优化，目标频率400hz
    [INFO] [1749363381.482384158] [dm_imu_node]: IMU data thread started
    [INFO] [1749363381.482655013] [dm_imu_node]: ACC: x=-0.207, y=-0.093, z=10.182
    [INFO] [1749363381.493355666] [dm_imu_node]: GYRO: x=0.000, y=-0.002, z=0.003
    [INFO] [1749363381.498234296] [dm_imu_node]: EULER: roll=-0.664, pitch=1.206, yaw=-22.287
    [INFO] [1749363383.488178712] [dm_imu_node]: ACC: x=-0.211, y=-0.128, z=10.253
    [INFO] [1749363383.509218697] [dm_imu_node]: GYRO: x=-0.001, y=0.003, z=-0.011
    [INFO] [1749363383.514222935] [dm_imu_node]: EULER: roll=-0.658, pitch=1.180, yaw=-22.290
    [INFO] [1749363385.488294066] [dm_imu_node]: ACC: x=-0.487, y=-0.145, z=10.205
    [INFO] [1749363385.509219435] [dm_imu_node]: GYRO: x=-0.001, y=-0.108, z=-0.619
    [INFO] [1749363385.514229967] [dm_imu_node]: EULER: roll=-0.674, pitch=2.727, yaw=-22.851
    [INFO] [1749363386.730293447] [dm_imu_node]: Processed 1000 packets, Published 985 IMU messages
    [INFO] [1749363387.488300414] [dm_imu_node]: ACC: x=-0.330, y=-0.105, z=10.205
    [INFO] [1749363387.509291897] [dm_imu_node]: GYRO: x=0.001, y=-0.012, z=0.006
    [INFO] [1749363387.514326904] [dm_imu_node]: EULER: roll=-0.569, pitch=1.898, yaw=-23.569
    [INFO] [1749363389.488344574] [dm_imu_node]: ACC: x=-0.296, y=-0.097, z=10.232
    [INFO] [1749363389.509350170] [dm_imu_node]: GYRO: x=0.004, y=-0.002, z=-0.000
    [INFO] [1749363389.514379616] [dm_imu_node]: EULER: roll=-0.619, pitch=1.649, yaw=-23.568
    [INFO] [1749363391.504368586] [dm_imu_node]: ACC: x=-0.280, y=-0.136, z=10.219
    [INFO] [1749363391.525306189] [dm_imu_node]: GYRO: x=0.002, y=-0.002, z=0.002
    [INFO] [1749363391.530329190] [dm_imu_node]: EULER: roll=-0.641, pitch=1.539, yaw=-23.577
    [INFO] [1749363392.064381282] [dm_imu_node]: Processed 2000 packets, Published 1985 IMU messages

    实现完成加速度，角速度，与欧拉角的快速收发

# 6月15日
## 痛点：CoordConverter变换过于复杂
1. <robotstatus>：状态枚举自定义消息转化，衔接一个接口逆向程序做转化，自定义消息转化为枚举
2. param文件重构，将原本的toml配置文件迁移为yaml文件，重构param程序进行参数热更新与读取
3. 基础filter和math模块做迁移，coord程序做迁移，适配自定义消息与新的param和基础模块——待测试
  <coord_converter>深入分析：
  ResistanceFuncLinear:含空气阻力的弹道计算类，后续会传给Ceres做自动求导

  <FrameBinder>:做帧绑定，在后续的Coord总类会声明一个实例来做时间戳与帧绑定.....?（疑似

  <PnpDistanceFixer>：PNP的距离解算修正（没找到用法

  <ShootParam与AimInfo>:嵌套的两个参数结构体:
    ShootParam：枪管和目标相机坐标
    AimInfo:封装上面的ShootParam，再加上YPD坐标的坐标和速度，还有一个ShootMode

  <Coord_converter>类：
    接收ArmorResult数据，接收图像，帧计数，还有四元数，时间戳

    声明了Eigen和CV下的三个矩阵，四元数，RobotStatus等成员变量
    声明帧绑定和Filter两个实例,一个绑定时间戳和帧，一个用于预测结束到发出去的时间延迟
    声明了一个RobotStatus的实例

    获取图像，帧计数，图像时间三个函数
    获取yaw,pitch，bullet三个常量
    
    11个延迟函数：
    img——>predict——>send——>control——>control——>fire——>hit：从图像开始产生一直到子弹发出去打击到对方整个延迟链路
    11个延迟函数，进行不同阶段的延迟获取，进行不同的组合计算阶段性的延迟总和，供给不同使用

    接下来是一段相机与陀螺仪坐标系的转换
    <get_camera_z_to_i>：
    做相机光轴投影到陀螺仪坐标系之下的变换，得到相机光轴在陀螺仪坐标系下的yaw和pitch角度
  
    然后是水平和垂直上的电控视觉差值补偿<aim_to_horiz_diff> (疑惑，没搞懂原理？？)

    两个误差补偿限制，一个是装甲板限制（理想装甲板的规格限制，不要跑出实际的界限），一个是理想点的限制，计算真实ypd和理想ypd的差距，然后矫正

    <aim_swing_cost>：计算发送的角度与偏差核算之后的值，如果是0刚好，代价计算函数
    <aim_cmp>：搞不懂，什么作用疑似判断优先级？？

    接下来是几个坐标系之间的变换手搓，包括含畸变的像素坐标pd,去畸变之后归一化坐标系pu,归一化到相机坐标系pc,再到陀螺仪坐标系pi，最后到枪管坐标系pi
    变换流程：
    图像坐标(pd) 
        ↓ pd_to_pu() // 去畸变
    像素坐标(pu)
        ↓ pu_to_pc_norm() // 归一化相机坐标
    相机坐标(pc)
        ↓ pc_to_pi() // 转到陀螺仪坐标系
    陀螺仪坐标(pi)


# 6月17日：
[终于搞懂^_^]:
1. 多个坐标系的变换本质上是完成世界坐标系的绝对角度转换，完成从相机坐标系到到陀螺仪坐标系的变换：

    图像检测 → PNP求解(相机系) → 坐标变换(陀螺仪系) → 云台控制
                              ↓
                          弹道补偿(枪管系)

    这样的好处是将仅相对相机坐标系识别的目标转换到世界坐标系，得到绝对角度，IMU自身知道自己转动的角度，将四元数传递给相机以后，安装位置固定的相机与IMU，相当于相机也知道自身的位姿了，看到世界坐标系0度的目标，15度转动的相机与IMU只需要转回去15度就能锁定目标

# 6月18日：
1. 重构core文件夹，新增deivce文件夹，存放相机和IMU驱动，IMU驱动新增零点校准，自定义设置当前IMU位置为世界坐标原点

2. 重新思考坐标变换和PNP程序：
    整理之后的变换思路：
      # 图像检测 → 像素坐标系 → 去畸变 → PNP求解转相机坐标系 → 转IMU坐标系 → 计算世界坐标系下的需要变化的角度 → 转枪管坐标系

    [测试]： 订阅IMU数据做帧和时间戳绑定测试角度计算

3. 现在要进行相机和IMU的外参联合标定，先标IMU和相机各自的内参，之后在统一联合标定外参“
    
  <IMU>:内参标定采取 Allan Variance ROS2 进行分析陀螺仪和加速度计的白噪声和随机游走
  
  之后使用kalibr进行联合标定


# 6月19日

...
1. IMU内参标定结束，随机游走与白噪声均表现良好，得到对应的imu.yaml

2. 失去所有手段…………，kalibr的仅支持ROS1，迁移到ROS2过于困难，新找到ikalibr，也仅仅是ROS1，暂时无法确定迁移难度，待定

# 6月23日

1. 成功完成IMU与相机的外参标定，现在可以进行坐标变换尝试将相机坐标系转为IMU坐标系，计算世界坐标系下的变换角度

2.  完成初版的解算节点，完整目标识别角度精度很高，但是如果目标不完整，角度解算出现畸变，距离也出现了问题
    问题：日志输出卡顿——解决：被距离解算给限制了，小于0.5的都被卡死了
    问题：距离解算出现极大的偏差——猜测：焦距调节了，内参失效了
    问题：针对不完全的识别目标会出现解算角度突变

    解决：角度解算与角点位置会有很大的关系，同时与距离强相关，优化标定多组内参，对于非完全识别目标进行过滤
    
    待测：YPD坐标系移植，弹道解算移植，瞄准参数生成移植，瞄准限制移植（装甲板与理想瞄准点限制），多组内参标定完善

# 6月24日
1. 多组内参标定：
    以60cm尺子为标准尺：
    [1]:最左端——焦点清晰极限无法确定，识别极限12cm
    [2]:最右端——焦点清晰极限在10cm左右,识别极限在18cm左右
    [3]:中间点——最远焦点清晰距离预计3m+,识别不受限
    [4]:中间～左端：焦点清晰极限无法确定，识别极限22cm左右
    [5]:中间~右端：焦点清晰极限30cm，识别极限其他50cm，哨兵35cm

  建议镜头焦距固定在中间点：可以应对30cm～3m的范围

2. 重新标定内参，PNP距离解算偏差问题解决，物体原始坐标尺寸要严格标注清晰，角度解算准确

3. 角度跳变不会产生了，但是0.1m与0.4m的识别目标，画面边界的计算的对应的yaw和pitch角度，差别竟然不是很大，仅仅相差0.5度左右，疑惑？？
    解答：相机的可观察视场角是固定的，水平与垂直的视场角固定，它测算的对应这个画面的边界角度不会有太大的误差，目标距离的远近对画面边界角度解算的影响不大

    经过计算，水平FOV视场角为42度，边界角度为19~21度之间，垂直FOV视场角为34度，边界角度为17~19度，角度解算精度很高 [^_^]

    垂直边界：[INFO] [1750752222.519492051] [pnp_solver_node]: 发布1个目标 - 最近: ID=4, 距离=0.18m, Yaw=3.5°, Pitch=14.8°
            [INFO] [1750752222.527939342] [pnp_solver_node]: 发布1个目标 - 最近: ID=4, 距离=0.18m, Yaw=3.5°, Pitch=14.8°

    # 加上IMU自身的偏差刚好20度左右
    水平边界：[INFO] [1750754933.577988375] [pnp_solver_node]: 发布1个目标 - 最近: ID=1, 距离=1.28m, Yaw=-18.6°, Pitch=-1.6°
            [INFO] [1750754933.588923852] [pnp_solver_node]: 发布1个目标 - 最近: ID=1, 距离=1.28m, Yaw=-18.6°, Pitch=-1.6°
            [INFO] [1750754933.597928456] [pnp_solver_node]: 发布1个目标 - 最近: ID=1, 距离=1.28m, Yaw=-18.6°, Pitch=-1.6°


# 6月27日

1. 
    # 装甲板几何约束
    加上样例装甲板的范围限制，确保解算得到的角度始终瞄准在装甲板中心点：
    原理：对识别的目标调用其对应的装甲板原始坐标比例去判断计算得到的瞄准参数与实际的应该有的yaw和pitch角度进行判断，给一个容错空间，确保容错空间的下界为装甲板边缘

    新增了一个几何约束补偿器<geometric_compensator>，确保能够保证解算的角度始终正对装甲板中心，对yaw和pitch补偿，距离补偿不如不补偿，更远了
    精度来到了百分位
    [INFO] [1751025462.254761591] [pnp_solver_node]: 发布目标: ID=4, 类型=small, 距离=0.218米, Yaw=18.52°, Pitch=-1.88°, 位置=(0.206, 0.069, -0.007)米
    [INFO] [1751025462.268544340] [pnp_solver_node]: 发布目标: ID=4, 类型=small, 距离=0.237米, Yaw=18.48°, Pitch=-1.62°, 位置=(0.225, 0.075, -0.007)米
    [INFO] [1751025462.290321168] [pnp_solver_node]: 发布目标: ID=4, 类型=small, 距离=0.237米, Yaw=18.23°, Pitch=-1.61°, 位置=(0.225, 0.074, -0.007)米



2. 先将瞄准解算得到的参数整理一下：
    整理之后得到较为简单的参数组成：
    string id                    # 装甲板编号（如"1", "2", "3"等）
    string armor_type           # 装甲板类型（"small" 或 "big"）
    # 3D位置信息（世界坐标系，单位：米）
    geometry_msgs/Point position
    # 角度信息（相对于初始IMU状态）
    float64 yaw                 # 水平角度（弧度）
    float64 pitch               # 俯仰角度（弧度）
    # 距离信息
    float64 distance            # 目标距离（米）

3. 弹道解算移植

   先考虑静止目标，弹道散布初始考虑为以装甲板对角线为直径的圆周范围，现在虽然瞄准参数直指装甲板中心，但是弹道的散布会让最终落点落在圆周范围内的随机一点，所以初始方案会加一个弹道解算
   尝试解决，至于如何解决随机的圆周散布，待思考


# 6月28日

1. 弹道解算移植成功，最短补偿启动阈值设定为0.506m
   [补偿]：
    [INFO] [1751088091.681630584] [pnp_solver_node]: 弹道补偿 - 目标ID: 4, 距离: 0.51m
    [INFO] [1751088091.681694407] [pnp_solver_node]:   世界坐标系补偿前: Yaw=6.977°, Pitch=4.898°
    [INFO] [1751088091.681707713] [pnp_solver_node]:   世界坐标系补偿后: Yaw=7.063°, Pitch=6.572°
    [INFO] [1751088091.681718430] [pnp_solver_node]:   世界坐标系补偿量: Yaw=0.086°, Pitch=1.673°
    [INFO] [1751088091.681729241] [pnp_solver_node]:   发射仰角: 4.659°, 飞行时间: 0.032s
    [INFO] [1751088091.681749765] [pnp_solver_node]: 发布目标: ID=4, 类型=small, 距离=0.514米, Yaw=7.06°, Pitch=6.57°, 位置=(0.508, 0.062, 0.044)米

2. 考虑是否建立补偿射表？..

3. 准备搭建状态管理池，对识别的目标建立映射表，映射到不同的种类，分为前哨站，基地与兵种，每个种类会有自己的运动模型，样例装甲板，实际的识别信息ID等，实际状态管理池更多的有待商榷
    # 为了搭建状态管理池，需要把识别输出数据修改一下
    Armor_result新增目标ID和检测装甲板数量，这样一个检测目标就会对应一个Armor_result，里面还会封装着过滤之后的armors
    为了应对多个检测目标，避免全部使用一个话题发布造成数据遗漏和竞争， 现在创建多个话题，为每个目标都创建一个话题，只存储对应的数据，解耦发布

<!--   # 话题数据示例
    timestamp:
  sec: 1751107117
  nanosec: 886286225
  target_id: 1
  armor_count: 2
  armor:
  - header:
      stamp:
        sec: 1751107117
        nanosec: 859650975
      frame_id: ''
    corners:
    color: red
    conf_color: 0.6579857468605042
    number: 1
    conf_number: 9.423467636108398
  - header:
      stamp:
        sec: 1751107117
        nanosec: 859650975
      frame_id: ''
    corners:
    color: red
    conf_color: 0.5818684697151184
    number: 1
    conf_number: 7.432125568389893
 -->
    还是采用单话题全量数据发布，对比了三种数据格式，
    多话题解耦数据发布，缺点是一个模块可以，但是多个模块处理的时候会造成话题数量爆炸式增长
    单话题组合发布，将armorresult进一步封装[]，加大数据结构复杂程度
    直接采用单话题全量数据发布，映射到状态池内分配创建线程对象，管理维护以后输出更高质量的数据

# 6月29日：
1. 状态管理池搭建，现阶段只实现了一个短暂识别丢失的模块，针对装甲板受击打出现的灯条闪烁，可以保证闪烁装甲板的有0.3秒的存活周期，装甲板对应的兵种有0.5秒线程的存活时间，定期检查存活线程与清理死亡线程
    但是当前线程机制仅仅能保证对象依旧存活，但是实际目标位置的角点像素都是靠历史变动轨迹来推测，没有上滤波器去准确的估计目标角点位置

    当前状态管理机制简单解释就是
      识别数据：
        目标ID: 3（步兵3号）
        装甲板数量: 2
        装甲板1: 位置(100,200), 编号1, 置信度0.9
        装甲板2: 位置(300,200), 编号3, 置信度0.8

    那么状态管理器就会为每个装甲板都分配一个唯一的线程ID，来保证当前装甲板有一个唯一识别身份，线程管理器会根据预设好的时间周期去完成创建，判活，杀死的循环，确保有稳定长期连续的装甲板数据，在0.3秒的丢失时间内，管理器会保存最后一次出现的位置，如果0.3秒内，如果装甲板重新出现在附近（最后出现的像素点位置的100像素内），就认为是同一块装甲板

<!-- 2. 建立双层ID标识，armor_result有target_id来帮助确定当前数据是哪辆车，还有armor_count来表明当前识别的装甲板数量，然后修改armor自定义消息，新增thread_id来为每一个装甲板维护独立线程，这样画面中的所有装甲板就有唯一ID分配，这样后续的预测器也会有稳定严格的数据传入,也会帮助确定到底打击哪个装甲板

    <eg>:
      时间 T0:
      Armorresult {
        target_id: 3
        armor: [
          {corners: [...], number: 1, conf: 0.9, thread_id: 0},
          {corners: [...], number: 3, conf: 0.8, thread_id: 1}
        ]
      }

  问题：当前的实现的thread_ID分配导致的问题是贪心竞争，线程都是独立的寻找竞争匹配最佳的目标，如果某个目标被发现是最佳匹配对象，那么就会被占据，导致原始的匹配对象被赶走，被迫创建新的线程，导致thread_id持续跳变，如果出现了快速移动，相邻帧的位移超过了匹配阈值就会新的线程不断创建，导致ID爆炸

  <eg>:
  第一帧：
  线程0 → 装甲板A (位置100,100)
  线程1 → 装甲板B (位置200,100)

  第二帧（装甲板位置略有变化）：
  新装甲板A' (位置102,100)
  新装甲板B' (位置198,100)

  贪心匹配过程：
  线程0寻找最佳匹配 → 发现B'更近（96像素） → 匹配B'
  线程1寻找最佳匹配 → A'已被标记匹配 → 创建新线程2匹配A'
  
思考：想要唯一ID分配去帮助预测器独立预测，但是没有预测器的辅助，对于移动或者小陀螺目标又无法很好的解决，需要运动模型去帮助判断当前目标的运动状态，陷入了一个死循环

解决：经过探讨，我们发现唯一稳定的装甲板ID其实并不重要，虽然可以锁定某一个装甲板，但是想要在复杂场景下分配这个稳定的ID，是极其困难的，因为物体移动了，角点像素的变化是非线性的，那么这时就需要稳定的运动判断和稳定的预测器去辅助才能确保为装甲板分配稳定的ID,
这需要将运动模型，预测器，状态管理系统紧密的耦合为一个模块才能解决问题，而且实际打击装甲板并不需要专门打击固定的某一块，而是打击得分或者说最容易打中的那一块，所以双层ID管理可能废弃 -->

3. 对装甲板估计进而实现对整辆车实现运动状态估计：（开搓运动模型）🐶 **狗头**

    **滤波器移植**
        移植滤波器和基础的math工具函数，支持单变量滤波器，3D位置滤波器和角度滤波器，在EKF和kalman的基础上进行封装，结合math程序

    重新思考了一下数据处理流程，识别->解算（仅调用到弹道解算之前的解算部分）->状态池->映射到每个兵种上->短暂识别丢失维护+调用映射运动模型->核心预测节点，调用运动模型对应的滤波器->之后传回PNP解算节点进行弹道解算->通讯端发送数据


# 6月30日

1. 小设想：对解算端的Target数据进行收集，尤其是position，三维坐标位置，对数据进行收集，以此来分析运动状态，可能有奇效
    **简单实现**：简单实现了一个运动分析节点，确实可以通过分析三维坐标位置和去完成对运动状态的判断，但是必须有严格的判断和审查才能确保进行精准的运动状态分析

    经过测试观察，实际上不需要过于复杂的运动状态判断，因为静止本身就能很好的打到，那么静止与常规运动可以放在一起，也完全不需要麻烦的去估计常规运动状态，只需要一个小陀螺状态的判断，识别到小陀螺就选择小陀螺模型，反之就是常规运动模型
    至于常规运动模型，调用一个PositionEkf去估计位置和速度，只需要传入目标的ypd坐标，然后设置好r_vec和t_vec，就可以进行运动估计，所以现在需要重构之前的common_model，把解算得到的数据（截至到弹道解算之前的数据传出去），
    给到运动模型，去调用对应滤波器进行预测

2. 重构了common_model，整理了一下整个结构，重写了对外接口，确保可以更好的加入到状态池去管理映射
    对pnp节点进行修改，加入了去畸变角点，用于计算追踪预测时的装甲板面积用于选择最佳的预测目标
    已经搭建了一个初始的预测主节点，预计槽点会很多，要把整套链路搞清楚，什么时候预测，该不该信任预测，预测多长时间都全部搞清楚


3. 经过实践，临时搓的东西，一托答辩，还需要仔细修改！！！！
    刨根问底！！直接从PositionEkf去进行测试，目前来看效果竟然还可以，只是速度估算不出来，有希望，真是撞了它🏇狗屎运了
    调个参先


# 7月1日

1. # 数据分析
  录制解算和预测的话题数据为bag包，然后使用plotjuggler导出为csv文件，写了个统计分析程序，对数据进行配对分析


可以看出，三轴角度虽然平均值可以接受，都是0.1分位，但是极差很大，yaw轴最大6度，pitch轴最大10度，三轴距离误差也达到了个位数cm精度，所以降低预测时间，调小到50ms，效果好很多


预测时间调小到50ms，精准度大幅上升，三轴误差精度最大值来到0.1cm精度量，平均值达到0.01cm精度级别，distance误差最大值还是有点大，但是也可以接受，yaw和pitch轴，精度最大值来到0.1度量级，平均值达到0.01度级  


| 测试条件 | X轴平均值(m) | X轴标准差(m) | X轴最大值(m) | X轴95%分位数(m) | Y轴平均值(m) | Y轴标准差(m) | Y轴最大值(m) | Y轴95%分位数(m) | Z轴平均值(m) | Z轴标准差(m) | Z轴最大值(m) | Z轴95%分位数(m) | 欧氏距离平均值(m) | 欧氏距离标准差(m) | 欧氏距离最大值(m) | 欧氏距离95%分位数(m) | YAW平均值(°) | YAW标准差(°) | YAW最大值(°) | YAW95%分位数(°) | PITCH平均值(°) | PITCH标准差(°) | PITCH最大值(°) | PITCH95%分位数(°) |
|----------|--------------|--------------|--------------|-----------------|--------------|--------------|--------------|-----------------|--------------|--------------|--------------|-----------------|-------------------|-------------------|-------------------|---------------------|--------------|--------------|--------------|-----------------|----------------|----------------|----------------|------------------|
| 25cm静止500ms | 0.0053 | 0.0056 | 0.0445 | 0.0165 | 0.0006 | 0.0009 | 0.0206 | 0.0020 | 0.0009 | 0.0015 | 0.0338 | 0.0025 | 0.0055 | 0.0058 | 0.0446 | 0.0168 | 0.16 | 0.27 | 6.53 | 0.49 | 0.17 | 0.41 | 10.64 | 0.43 |
| 25cm静止50ms | 0.0043 | 0.0061 | 0.0370 | 0.0195 | 0.0003 | 0.0005 | 0.0033 | 0.0012 | 0.0005 | 0.0007 | 0.0044 | 0.0020 | 0.0044 | 0.0061 | 0.0373 | 0.0196 | 0.05 | 0.09 | 0.64 | 0.23 | 0.04 | 0.07 | 0.51 | 0.23 |
| 50cm静止50ms | 0.0146 | 0.0118 | 0.1009 | 0.0365 | 0.0011 | 0.0015 | 0.0169 | 0.0031 | 0.0013 | 0.0028 | 0.0470 | 0.0029 | 0.0150 | 0.0119 | 0.1034 | 0.0368 | 0.12 | 0.17 | 1.72 | 0.39 | 0.14 | 0.34 | 6.18 | 0.32 |
| 1m静止50ms | 0.0124 | 0.0259 | 0.2566 | 0.0770 | 0.0024 | 0.0053 | 0.0406 | 0.0155 | 0.0008 | 0.0019 | 0.0236 | 0.0041 | 0.0127 | 0.0265 | 0.2599 | 0.0786 | 0.02 | 0.06 | 0.65 | 0.15 | 0.02 | 0.11 | 1.60 | 0.08 |
| 1m静止300ms | 0.0349 | 0.0490 | 0.2847 | 0.1420 | 0.0026 | 0.0043 | 0.0286 | 0.0121 | 0.0037 | 0.0049 | 0.0378 | 0.0133 | 0.0356 | 0.0492 | 0.2859 | 0.1439 | [0.06] | 0.13 | 1.10 | 0.26 | [0.07] | 0.12 | 1.60 | 0.23 |
| 1m静止500ms | 0.0283 | 0.0464 | 0.6198 | 0.1352 | 0.0025 | 0.0044 | 0.0731 | 0.0133 | 0.0014 | 0.0028 | 0.0433 | 0.0062 | 0.0286 | 0.0466 | 0.6243 | 0.1358 | 0.03 | 0.10 | 1.15 | 0.16 | 0.04 | 0.12 | 2.43 | 0.19 |
| 0.5m匀速500ms | 0.0665 | 0.0747 | 1.1239 | 0.2328 | 0.0173 | 0.0180 | 0.0964 | 0.0560 | 0.0150 | 0.0166 | 0.3083 | 0.0478 | 0.0769 | 0.0722 | 1.1646 | 0.2376 | 1.88 | 2.11 | 12.48 | 6.35 | 1.98 | 2.16 | 11.34 | 6.85 |
| 0.5m匀速300ms | 0.0713 | 0.0675 | 0.3543 | 0.2051 | 0.0096 | 0.0136 | 0.0828 | 0.0437 | 0.0187 | 0.0183 | 0.0933 | 0.0544 | 0.0801 | 0.0646 | 0.3647 | 0.2079 | 1.04 | 1.22 | 7.74 | 3.87 | 1.89 | 1.89 | 9.42 | 5.65 |
| 0.5m匀速300ms高响应10 | 0.0642 | 0.0603 | 0.3260 | 0.1907 | 0.0232 | 0.0223 | 0.1123 | 0.0713 | 0.0241 | 0.0215 | 0.0941 | 0.0697 | 0.0822 | 0.0556 | 0.3341 | 0.1995 | 2.46 | 2.36 | 12.73 | 7.53 | 2.83 | 2.51 | 9.93 | 8.00 |
| 0.5m匀速300ms低响应200 | 0.0820 | 0.0838 | 1.2841 | 0.2705 | 0.0131 | 0.0126 | 0.0895 | 0.0397 | 0.0144 | 0.0163 | 0.4447 | 0.0438 | 0.0871 | 0.0835 | 1.3618 | 0.2739 | 1.43 | 1.33 | 9.28 | 3.81 | 1.62 | 1.47 | 13.25 | 4.27 |


可以看到随着距离翻倍，明显误差翻了一倍，以95%分位数为标准，因为可能出现抖动造成偶尔的极差，待思考解决，降一降

<!-- 可以看出三轴与distance误差依旧翻倍 -->（傻逼了，距离翻倍，当然三轴与distance也翻倍，🍀），但是惊喜的是，yaw和pitch收敛竟然还不错，平均误差竟然来到了0.02度，与25cm时候差不多，95分位比25cm时候误差更小！！！

经过测试，在三个不同距离下静止收集数据测试，排除一些外来因素，平均误差达到可以接受的程序，现在进行数学推导计算可接受的最长预测时间,经过推导，我们估计得到1m静止情况下，yaw和pitch在预测时间0.3s的时候可能会达到1度误差，但是经过实际测试以后我们发现误差并没有提高很多，
平均值仅仅0.6度，最大值虽然都达到了1度左右，但是可以接受，这很惊喜，看来滤波器比我们想像的更加稳定


# 接着加！！！预测时间给到500ms，这个时间对于静止物体，即使是相距5m的距离也足够容纳弹道解算下的子弹飞行时间332ms也足够了，预感这个时间下，误差可能会加大了，但是如果在可接受范围内，那真是枪枪爆头好运连连

# 爽了，现在500ms的延迟也可以接受，误差低到可以忽略的程度，但是唯一的缺点就是distance有点误差过大，但是yaw和pitch都是可以接受的，distance误差大的情况希望正常解算的distance来助力一下，够用了500ms的误差足够容忍机械和电控的误差，不够的话也有一定预留空间


# 7月2日

1. 预测点图像观察：

  [全程静止]：
    针对全程静止的目标，收敛性很好，会有微微的漂移，但是都收敛在装甲板的范围内，加上弹道解算的部分刚好可以补偿抵销掉（弹道解算要加预测后偏移补偿了，加一点点）

  [匀速运动目标]：
     大多数情况收敛还算良好，但是尤其针对静止转运动和运动急停静止的时候，会有上一时刻的静止量和运动量带来的速度误差，这个误差会导致接下来一小段时间内收敛很差，考虑过滤这一小段时间的部分数据，不采取加入到滤波器数据中，减少误差

  [加速度运动]：
      适应性极差，不是预测，是尼🏇时光倒转，预测的点都是差一段跟上，这时候针对运动状态突变的情况下更是预测性能更是答辩

  问题：解决运动状态突变的时候带来的速度误差

  观察：不是预测时间的问题，而是滤波器本身的性能问题，对于运动状态突变不是很好适应，也不是数据单位的问题，调高速度先验方差到之前的5倍，有所缓解，但是似乎还是差点意思，调低yaw追踪的先验方差为之前的五分之一，尝试，有一点作用，但是效果不大，轻微匀速上下或者左右,测了一遍数据，
       调参之后也没什么长进


  验证了昨天的yaw和pitch在运动状态下的最大值就是运动状态突变产生的巨大偏差


2. 尝试测量精准的延迟时间，相机->检测推理->解算->预测的全链路延迟，经过测量，大部分时间，整个链路的延迟时间都在35～40ms之间，虽然预测精度针对运动状态突变的时候还是差点意思，但是似乎丝滑了很多，意外之喜！
    为什么丝滑：精准的预测时间，对应的是精准的位置与数据，30ms的硬编码延迟实际对应的位置可能并不是那个时间下的位置，所以会出现误差，精准时间带来的数据会帮助预测更加的可信任
    [INFO] [1751453912.328485275] [target_predictor_node]: 系统延迟: 30.47ms, 当前预测时间: 35.70ms
    [INFO] [1751453913.331656327] [target_predictor_node]: 系统延迟: 33.85ms, 当前预测时间: 33.06ms
    [INFO] [1751453914.332711022] [target_predictor_node]: 系统延迟: 34.65ms, 当前预测时间: 35.16ms
    [INFO] [1751453915.376657599] [target_predictor_node]: 系统延迟: 34.20ms, 当前预测时间: 36.56ms

    测了数据，还是没什么长进，与预测时间的关系还是没有那么重要

3. 新发现，之前的预测端数据是使用世界坐标下的posistion，加一个转换函数去作为数据来源，现在直接使用解算端的ypd作为数据来源去测试会不会有改进，收敛确实更加快速了
    **新思考**：可以为当前的滤波加上一定时间的采样时间限制，确保可以遗忘一些过去的坏数据，期望能够避免运动状态突变带来的速度误差


# 7月3日：

1. 针对运动状态突变的EKF的思考解决方案：
    <1> 
        采用创新序列做运动状态突变的检测,然后在检测到运动状态突变的时候，动态调整对旧数据与新数据的适应程度，以期望抵销掉运动状态突变带来的速度误差进而导致的位置误差，此外还期望加入采样时间限制，以期望通过限制采样时间来遗忘过往的坏数据

    <2>
        对滤波器进行升级，参考天大的更加多维的滤波器，加入到了角度θ与加速度做一个11维度的滤波器，
        V3版本的采用的
            [ x, y, z, theta, vx, vy, vz, omega, ax, ay, b ]  [ x, y, z, theta ]  传入观测量XYZ位置与目标的朝向既可推测出后面的更多的数据，预期可以解决当前滤波器的漏洞
        V4版本则是采用的
            [ x, y, z, v, vz, angle, w, a ]  [ x, y, z ] 传入目标的观测量XYZ位置即可估算出目标的曲线运动时候

    # 现在尝试迁移V4版本


2. 
  # V4版本
  令人震惊的收敛之后的精度，yaw和pitch收敛之后的平均值都来到了1度以下，这很令人震惊🤯
      YAW角误差:
    - 平均值: 1.00°
    - 标准差: 1.33°
    - 最大值: 23.60°
    - 95%分位数: 3.40°

  PITCH角误差:
    - 平均值: 0.73°
    - 标准差: 1.25°
    - 最大值: 20.24°
    - 95%分位数: 2.77°
    但是高精度也会有重大问题，那就是初始化收敛的速度略慢，大概需要2~3秒，这期间预测点会乱飞，这将导致极值变大，暂时还不知道这种情况会不会在切换目标的时候出现，以及这种初始收敛的是否会经常发生
    好吧，经过测试，我们发现，如果目标在未预测之前就出现在视野中被检测到，那么就不会出现长时间初始化收敛的问题，但是我们经过测试也发现当前目标消失，新的目标出现在视野中又会出现这种长期初始化收敛的情况
    现在我们调节初始化收敛所需的数据点数进行测试，经过测试不是数据数量的原因

  [调节]：
    修改了V4的初始化，初始化的时候不再估计角速度与加速度，收敛的十分的快，虽然数据上有进步，但是明显能感觉到，平均收敛的精度有一点下降
      YAW角误差:
    - 平均值: 0.94°
    - 标准差: 1.01°
    - 最大值: 8.23°
    - 95%分位数: 3.09°

  PITCH角误差:
    - 平均值: 0.60°
    - 标准差: 0.76°
    - 最大值: 4.78°
    - 95%分位数: 2.29°

  我们将收敛的过程省略，让目标一开始就出现在视野中，去进行估计，惊人的发现在不初始估计角速度和加速度的情况下，也比这种省略初始化的收敛的效果好：
    YAW角误差:
    - 平均值: 1.17°
    - 标准差: 1.10°
    - 最大值: 7.78°
    - 95%分位数: 3.44°

  PITCH角误差:
    - 平均值: 0.76°
    - 标准差: 0.87°
    - 最大值: 5.31°
    - 95%分位数: 2.69°

  [新发现]：
    在我们去掉初始估计加速度和角速度的情况下，虽然它在近处表现的收敛精度很好，且收敛迅速，但是当目标过远，目标过小的情况下，收敛就会出现振荡，精度会迅速下降，现在考虑重新加上初始估计加速度和角速度尝试，差不太多，都是精度会下降，现在数据收集验证一下
    1. 

    YAW角误差:
      - 平均值: 2.82°
      - 标准差: 12.61°
      - 最大值: 177.36°
      - 95%分位数: 4.26°

    PITCH角误差:
      - 平均值: 0.42°
      - 标准差: 0.68°
      - 最大值: 19.66°
      - 95%分位数: 1.60°

   2. 
      YAW角误差:
      - 平均值: 0.71°
      - 标准差: 0.78°
      - 最大值: 5.44°
      - 95%分位数: 2.38°

      PITCH角误差:
        - 平均值: 0.41°
        - 标准差: 0.55°
        - 最大值: 5.42°
        - 95%分位数: 1.55°
  
  # 总结，针对3～4m以外的目标，受限于目标过小，可能识别受限，导致解算预测都受到一定影响，但是我们惊奇的发现，在更远处，更加晃动的目标，收敛似乎更好了，V4当前版本足够作为备选滤波器模型，已经达到可以使用的要求,对V4做的去掉初始估计角速度和加速度的优化保留


3. 现在尝试迁移V3版本：

    依旧震惊，收敛精度性能更好，超过了优化之后的V4版本，仅仅使用position就达到很好的收敛精度，虽然面对远距离的小目标还是会有延迟，但是已经很好了，而且没有收敛速度慢的缺点，切换目标的情况还待测试

    YAW角误差:
      - 平均值: 0.65°
      - 标准差: 0.69°
      - 最大值: 5.75°
      - 95%分位数: 2.03°

    PITCH角误差:
      - 平均值: 0.46°
      - 标准差: 0.67°
      - 最大值: 7.43°
      - 95%分位数: 1.90°

  # 总结，V3版本比V4滤波器性能更加强劲，且不会有初始化收敛的问题，将作为主要的滤波器去使用，针对静止，匀速，匀加速平移的目标都有较好的适应性


4. 现在封装滤波器为常规运动模型，封装为简易的接口，方便在后续映射中直接调用,封装测试成功，现在只需要调用简单的接口就能进行预测，现在思考如何应对多个目的追踪
    初步的思路是相同ID目标追踪打击精准度更高的（这里小陀螺和常规平移可能不同），然后不同ID选择优先级级别高的

    # 思考后的决策结构：
        对识别端重新整理，按照机器人本身优先级去排序发布armor_result，然后armor_result本身按照多级排序去填充armor
        数据发布以后，预测端优先对打击难度低进行唯一跟踪，即识别到英雄与步兵，只预测他们两个理论上四个装甲板中最好打击的两块，然后选择性的发布英雄或者步兵
        
        打击难度的考虑：目标处于何种运动状态，小陀螺还是常规平移，距离是远还是近，装甲板的朝向是正面还是倾斜

   先修正一下识别端的数据排序


**以及AI为我提供了一个忽视的一点，就是运动过程中的弹道解算，以及整个车辆处于运动过程中，解算，预测等等诸多模块会不会受很大的影响！！！！**
        
**动态优先级与未来打击难度的预测后续需要考虑**


# 7月5日

1. 修复了一个遗留的历史bug，狗屎的几何补偿器会导致在解算时定期的丢弃一些倾斜角度过大的装甲板，这就导致解算数据的时间不连续，不是连续的稳定的时间序列数据将会导致滤波器在处理预测的时候不会长期稳定，会出现预测延迟的问题
   **另一个问题**：当装甲板的倾角过大，即pitch和yaw角度过大的时候，常规的平移模型显然无法继续胜任了，预测此时会停止，所以小陀螺专用的滤波器还是很有必要的

2. PNP解算距离还是出现了问题，目前测试在1.5m处是极限，这时候的误差还是在2～3cm之间，但是这个时候不管是相机左右转动还是装甲板左右转动，不是正直冲向相机，都会影响解算的距离，当达到2m左右，
    此时左右晃动解算得到的距离就会出现20~30cm左右的偏差，而且本身正直冲向相机累积的误差也达到了8～10cm

    一些思考的解决方案：收集不同距离下的不同角度的解算距离偏差，搭建一个补偿函数，然后加一个计算朝向角的判断标准，先计算朝向角，然后再
    进行补偿，计算朝向角采取三分法的PNP，或者计算边界框的两个对角线的长度比以及对边的收敛程度

    **遗漏点**:之前在做坐标变换的时候为忘记思考PNP求解得到的r_vec实际上包含装甲板自身的倾斜角度的，这就导致后续这个装甲板自身角度倾斜的误差会累积到后续一整个流程中，解算距离会出现偏差，预测也会跑飞
              距离相机正对着装甲板，此时解算位于世界坐标下的-5度，然后此时左转大概30度，这时位置不变情况下，解算得到的世界坐标系角度为-3度，右转30度，此时的角度时-7度，正负4度的误差，随着距离增大，这个误差更大，现在考虑能不能在三分法PNP的基础上分离
              装甲板自身倾角和装甲板的世界坐标系下的所处角度


    **思考整理**： 想像当前世界坐标系为一个圆规，目标正对着相机位于90度的位置，这时位置，距离都是准确的，但是如果还是位于90度，但是自身倾角10度，这时候传统PNP就会把你倾角的误差认为是正常的，它会解算认为你现在位于85度位置，然后此时解算距离也是不准的，我们想要的效果是
                知道目标还是在90度位置，然后倾角也知道了，解算距离也准确，YawPnP的效果是认为它在85度，然后它的朝向角也是计算准确的，因为之前的自身倾角可能是有误差的，所以现在还需要进一步优化


# 7月6日

1. 重新梳理了思路：
    在原本的YawPnP三分解算上修改，原本的YawPnP是只优化求解一个总的Yaw,当前我们的目标是分离世界Yaw和自身Yaw，所以我们思考以后设计了一个新的联合优化思路，当然联合优化求出精准的自身Yaw和世界Yaw，是很难的，搜索空间很大
    所以思考加入完整的约束
    <1>
      相机水平FOV—40度，装甲板稳定检测的自身Yaw偏转角度——正负30度，估计Pitch 15度，IMU世界坐标系（云台当前角度），r_vec旋转矩阵约束：R_total = R_relative × R_self
    <2>
      YawPnP的原本的几何边，点，与重投影约束
    加入这些以后，搜索空间大大降低，期望能够逼近原本算法的精度


# 7月11日

1. 腾出点时间改一下（doge
    梳理了之后我们发现PNP的r_vec本身就是不靠谱的，累积误差，不能以r_vec去分离姿态，而是独立求解目标的位置旋转矩阵与自身旋转矩阵，自身旋转矩阵中的yaw_self，还是采用交的思路，重投影计算，三分法迭代，基本思路就是将3D目标点投影下来，在搜索范围内，
    与固定的pitch和roll下，纯蒙yaw，什么时候投影下来的角点与检测图像重投影误差最小的时候，就是正确的yaw，得到了yaw_self，反推出目标的自身旋转矩阵，至于位置旋转矩阵，初步思路是通过position直接atan2计算yaw和pitch

    时间长不看，原本的yaw和pitch就是用position加atan2计算的，但是误差还是存在，看来自身倾斜还是会影响t_vec

# 7月12日 

<!-- 1. 考虑思考新建装甲板坐标系了，这样才能解算得到目标的法向量和真正的几何中心，用来估算真正的世界坐标系所处角度，t_vec还是耦合了姿态和位置，暂时不能用 -->


2. 对分类重新限制了一下，限定大小装甲板，大装甲板只搜索1和2,小装甲板只搜索3，4，哨兵，选置信度最高的，避免一些误识别，，有效果，但是还是得炼一下分类

3. 
  **茅塞顿开！！！！**
  
    之前一直都掉入了完美的陷阱，总是执着于想要知道一切，即使装甲板旋转了，我也想要知道其精准的位置和自身旋转，但是完全没必要，小陀螺状态下装甲板是会旋转的，完全可以等到目标正对着相机
    朝向角最小的时候，这时候的PNP还是可靠的，可以使用，这时候就是打击时刻！！但是转念一向，其实需要打击倾斜装甲板的情况还是存在的，因为有时候装甲板会从视野中斜向走过，这时候之前一直纠结的问题还是会出现，需要看一看比赛
    来确定这种情况是否常见

    现在还是要计算一下朝向角，能够帮助辅助计算最佳的打击时间

4. 梳理总结了一下当前朝向角的计算流程，输入的数据是四个2D角点，PNP解算的三维位置，以及armor_type，然后这里定义了两个向量，一个是装甲板向外的法向量，一个相机的z轴向量，把两个向量投影到xy平面，注意这里，相机z轴向外，投影下来是0度向量，
   法向量正对相机的时候，投影下来是180度，这里做一个180度差值相减，这个差值为0的时候，补偿z轴转动的角度就是朝向角，开始假设朝向角，然后计算对应的三维角点，然后投影到图像上，计算像素，边长，角度联合代价，采取三分法搜索这个假设角度，代价误差
   最小的时候，就是最佳理想的yaw角

   已经测试，精度不是很高，数据波动也很大，但是有时又很准，但是如果面对小陀螺的目标，还真能很好的判断出来，现在思考如何优化......

   优化思路：
  <1>
      加入一些矩形固有的约束，比如水平，垂直边交比，以及对角线交点
  <2>
      还要考虑不同距离下的朝向角的计算补偿，因为现有解算距离似乎1.8m就是极限了，再远处就会出现解算距离不准确的问题，至少在2.2m的距离下，解算得到的距离是2m，这误差很大了已经，所以还得思考如何补偿解算距离


# 7月13日

1. 先思考如何补偿PNP解算距离，对不同目标下的识别距离与解算距离都记录下来，记录一段时间，拟合出对应的补偿参数，然后考虑根据检测框像素差作为启用依据，去启用不同距离下的参数：
    <fixed_distance = a0 + a1 × distance + a2 × distance²>

    拟合出对应的这个a2,然后补偿计算

2. 延续了我们之前思考的交比与对角线对点，我们对矩形的投影变换进行了深入的分析，发现很多新思路，意外之喜！！
    除了交比以外，还发现了消失点法和单应性矩阵求解，只不过这些方法都得pass掉了，因为YOLO的粗框选得到的检测框并不能提供精准的角点数据（留给后人优化了 doge
        YOLO检测框：
    ┌────────────────┐ ← 这些边延长永远是平行的
    │   /────/       │
    │  /    /        │ ← 真正倾斜的矩形在里面
    │ /    /         │   但我们不知道它的准确边缘
    │/────/          │
    └────────────────┘ ← 这些边延长也永远是平行的

    又有新思路，当装甲板倾斜的时候，边界框逐渐由正方形变换为长方形，这时候变化明显的是边界框的对角线，以及内切圆外接圆，最佳的方案就是对角线辅助计算

    修改了重投影的误差计算，将原本的角点，边长，角度三个代价改为
        double aspect_ratio;          // 宽高比
        double diagonal_ratio;        // 对角线长度比
        double diagonal_angle;        // 对角线交角（弧度）
        double area;                  // 面积
        std::pair<double, double> diagonal_lengths;  // 两条对角线长度

    几个代价综合计算，针对大倾角的时候，比如35度以上，这时候检测框确实变为明显的长方形，此时估算的角度不一定准确，但绝对是大角度，但是针对小角度的时候，我们发现检测框变化程度其实并不明显，所以问题解决了一半


# 7月18日：

1.  对解算进行数据采集的时候，最远是1.5m左右就完全的不可信任了，虽然还偶尔能解算接近准确，但是计算精度大幅下降，然后是1.1m这时候是能够较为稳定的进行解算，在1.1和1.5m之间的这段距离，如果一直保证解算一个固定距离的目标，且前后不频繁
    变换距离，此时解算还是可以信任，这时候误差在4～8cm之间，但是如果你移动目标，比如上一段时间在1.1m处，接下来移动到1.2m，它的解算数据与1.1m解算的距离更接近，不是原本1.2m处应该有的距离，然后在1.1m以内，这个现象就没有那么明显，
    至于1.5m以外，误差则是20cm开外，完全不可信任

    [解决]： 看来需要修改检测了，传统的YOLO检测框粗框选太不稳定，检测的像素误差很大，看来需要优化检测端了，膜拜了天大的开源，准备迁移灯条检测配准模块，在检测端得到的检测框的基础上，对ROI区域进行提取，使用传统方法进行灯条提取生成四个角点
            有了稳定的角点才能进行稳定的解算和接下来的运动模型估计
      

2. 对检测端再次进行重构
    分为三个主模块，detector检测端，classifier分类端，filter过滤排序端，还有common定义了常用数据结构和工具函数，之后使用一个pipeline作为主节点去启动各个模块串联起来进行启动


# 7月19日：

1. 灯条检测配对，生成角点模块已经移植成功
    pointer模块：
      参考了天大的pointer的模块，对检测框的提取ROI区域，然后进行扩展ROI区域，之后进行灰度，直方图阈值，然后二值化计算，之后进行findContours查找所有轮廓，这里需要加一个检测框的限制，只在检测框内进行灯条的配对
      之后重心法计算灯条的端点位置，之后生成四个角点，检测灯条的时候考虑的参数选择灯条的长宽，面积，与角度，配对的时候考虑灯条之间的角度差，面积比，长度比，宽高比，以及灯条与检测框的最小重叠比例（确保灯条检测与配对始终在检测框范围内进行）

    [测试]：
        对灯条和检测框的角点像素波动误差进行测量，以此来确定是否灯条角点更加稳态
        对数据进行采集后发现，灯条的角点更加稳定，虽然在静止相机和静止目标的情况下还是有像素误差偏移，但是所幸我们得到了比检测框更加真实且稳定的角点
        --- Armor Corners Analysis ---
        Center Point Jitter:
          X-axis std: 2.730 pixels
          Y-axis std: 0.588 pixels

        Frame-to-Frame Changes:
          Mean displacement: 4.603 pixels
          Max displacement: 247.625 pixels

        --- Light Corners Analysis ---
        Center Point Jitter:
          X-axis std: 1.140 pixels
          Y-axis std: 1.003 pixels

        Frame-to-Frame Changes:
          Mean displacement: 1.853 pixels
          Max displacement: 243.937 pixels

2. 现在重构解算节点，把之前的检测框节点改为现在的灯条节点，看看解算是否会更加稳定
    <!-- 重构解算节点之后进行了一次解算的距离的数据收集，目前稳定识别的极限是3.2m,这时候即使调节曝光也无法达到长时间稳定的识别，然后识别的极限是4.4m，但是解算的距离会有巨大的跳变，此时数据不可信任
    同时随着目标距离的远离，误差逐渐变大且随着距离变大，误差也以一种线性增加，最大的误差达到13cm,现在根据收集的数据建立补偿函数

    [实测]        [解算]
    40cm          39.7cm
    50cm          49.2cm
    59cm          58.9cm
    69cm          69.7cm
    78.5cm        79.4cm0
    88cm          89.8cm
    97.5cm        99.8cm
    107cm         110cm
    117cm         121cm
    127cm         131cm
    136cm         141cm
    151cm         156cm
    162cm         168cm
    171cm         177.5cm
    183cm         190cm
    191cm         198cm
    202cm         209cm
    211cm         218.5cm
    221cm         229.6cm
    232cm         240.3cm
    242cm         249.7cm
    252cm         261cm
    262cm         270cm
    272cm         281.7cm
    282cm         292cm
    292cm         305cm
    303cm         316cm
    312cm         321cm
    
    拟合得到二次多项式函数： Formula: actual = 0.000052 × calc² + 0.9386 × calc + 3.21 -->

  突然发现好像误差没那么大了，而且之前拟合的函数数据太少，用来做补偿以后误差更大了，回退版本了，暂时先不补偿了

3. 还需要分析一下装甲板有自身倾角的时候为什么无法检测配对灯条，什么原因阻止了？
    经过测试输出日志发现，限制检测的主要原因检测到的灯条轮廓长宽比过大，所以经过测试验证，将最大长宽比提高到20,这时候检测的自身倾角极限是正负45度左右，如果角度再大，此时灯条会因为过于细长而直接消失在视野中，所以再提高也没用，45度足够了
    轮廓#7:
    中心: (274.984, 146.002)
    尺寸: 109.16 x 7.5823
    角度: -7.35239°
    长度: 109.16
    长宽比: 14.3966
    面积: 827.68
    [拒绝] 基础检查失败: 长宽比过大(14.396624 > 8.000000); 

  之前被朝向角计算折磨了很久，主要是YOLO检测框的角点就是答辩，不稳定的像素抖动，以及不是真实角点会导致朝向角计算十分不可靠，所以现在将使用最新的灯条生成的角点去进行计算

4. 朝向角计算重构：
    回退版本成功，还是之前的角点像素，边长以及角度的重投影误差解算，现在可以达到很精准，转动的左右角度正负是准确的，使用了指南针进行校准，计算的极限观测角度与之前预想的一样，就是正负45度，在小倾角的时候会有几度的延迟和跳变，在大角度的时候
    很好用，虽然可以进一步优化，但是目前来说够用

  <!-- 
      处理后朝向角(zn_to_v): -24.6167 度
      处理后朝向角(zn_to_v): -26.1464 度
      处理后朝向角(zn_to_v): -27.8576 度
      处理后朝向角(zn_to_v): -29.5098 度
      处理后朝向角(zn_to_v): -30.8281 度 -->


5. 重新串起整个流程，开了预测器，角点的精度提升对于预测器没有质的飞跃，还是会出现漏跟踪的情况
    [情况总结]
      1. 上下运动，此时装甲板出现大仰角，就会出现掉预测的情况，这是正常的，实际场上不会出现这种pitch轴上的大角度的情况，所以暂时不用解决
      2. 远距离出现延迟，虽然还是能跟上，但是这时候本身受限于远距离角点的像素太小，检测不稳定，后面整个链路都会出现延迟
      3. 从远处冲向相机或者远离相机，以及在相机视野左右平移运动，都可以跟得上
      4. 斜向行走，慢速还是可行，但是速度快了就会掉预测
      5. 行走中途变向，无法做到稳定的响应变向，但是在变向稳定之后会快速的响应跟上
      6. 原地钟摆晃动，也是慢速能跟上，但是快速就会掉预测


  之前common_model仅仅使用position来作为观测向量，现在思考加入角度，也就是朝向角，看看效果是否会好一些

  // 1. 位置和角度直接使用最新观测
  state(0) = p3(0);  // x
  state(1) = p3(1);  // y
  state(2) = p3(2);  // z
  state(3) = p3(3);  // theta
  
  // 2. 速度通过差分估计
  double dt2 = p3(4) - p2(4);  // 时间差
  state(4) = (p3(0) - p2(0)) / dt2;  // vx = Δx/Δt
  state(5) = (p3(1) - p2(1)) / dt2;  // vy = Δy/Δt
  state(6) = (p3(2) - p2(2)) / dt2;  // vz = Δz/Δt
  
  // 3. 角速度通过角度差分估计
  double theta_diff = reduced_angle(p3(3) - p2(3));
  state(7) = theta_diff / dt2;  // omega = Δtheta/Δt
  
  // 4. 加速度通过二阶差分估计
  double vx1 = (p2(0) - p1(0)) / (p2(4) - p1(4));
  double vx2 = state(4);
  state(8) = (vx2 - vx1) / (p3(4) - p1(4));  // ax = Δvx/Δt


# 7月20日

1. 现在有了较为精准的朝向角计算，升级之前的运动模型，允许角度加入观测，可以计算角速度
    虽然有了朝向角数据，但是考虑到我们没有十分稳定的长期不出错的数据，所以对之前的滤波器进行双轨升级，如果有稳定的朝向角数据，就采取4D观测，否则则降级为3D观测，注意这里的降级衔接要做好，思考了两个方案，一个短时间的数据记忆保留，丢失朝向角数据的时候，保持0.5秒的数据
    另一个是针对EKF本身入手，调高角度相关状态的过程噪声，让EKF自行处理

    [测试]：
    1. 经过测试，现在的双轨预测，几乎百分百时间都是在4D预测，推测是灯条存在能够成功检测到就能解算计算朝向角，所以几乎所有时间都有朝向角数据，然后经过测试后发现，初始化时会有一段时间的收敛，猜测是加入了朝向角以后的原因，但是相比于之前测试过的v4，它可以做到在目标丢失以后再出现还能快速响应，所以还是有一定进步的

    然后经过测试，原地与前进的钟摆运动，绝大多数时间都可以适应与追踪的很好，变速突然的前进，以及前进途中转向也可以响应的很好，原地慢速小陀螺也可以响应，虽然加了角度观测以后似乎振荡性有所增加，但是适应性加强了，而且对于纯平移左右运动与前后运动，适应性反而降低了，初步思考是当前的xyz速度与朝向角解耦，没有有朝向角的情况下的速度方向的变化

    <!-- 2. 思考之后的新的解决方式，考虑为装甲板本身新建一个本体坐标系，然后在运动模型中获取滤波器估计预测的数据，然后将其中的受到朝向角噪声污染的速度数据提取出来，转换到本体坐标系下进行一段较短时间的数据分析，转换是将速度分解为两个方向的速度，分别是沿着朝向角的前推速度以及垂直朝向角的平移速度
    R_world_to_body = [ cos(θ)   sin(θ)   0]
                      [-sin(θ)   cos(θ)   0]
                      [   0        0      1]

    分解完速度以后，去分析一段时间，根据装甲板坐标下的速度去调整滤波器中的参数 -->

    3. 先去掉4D与3D的双轨，测试发现几乎一直是4D预测，之前的装甲板自身坐标系的想法就是答辩，回退版本

2. 经过测试观察发现，当目标倾斜的时候，PnP距离也会随之增加，这个偏差的距离与朝向角相关
    Δr ≈ 67.5mm × sin(θ)  （装甲板半宽）
    此时测算得到的距离是最远的角点到相机的距离，这会帮助解决滤波器不稳定的重要参考，但是因为PnP距离的解算与距离正相关增长的误差，以及这个倾斜偏差距离也与距离正相关增长

    看来还是需要进行逐点的校准补偿误差


3. 先尝试实现别的组件[反小陀螺]：

    参考天大的小陀螺模型，采取以单板为输入估计计算整车的滤波器，估计目标整车的半径，以及xyz车辆中心坐标

    已经成功一直反小陀螺模型，并从小陀螺滤波器中找到了新的思路，小陀螺滤波器创新性的启用了三个滤波器，主要的9维滤波器，KF的角度和位置滤波器，三个滤波器同时启动进行观测预测

    所以新思考针对之前的common_model，常规运动模型的滤波器也可以考虑启用多个滤波器来进行同时观测

    初步测试效果还是不错的

# 7月21日

1. 尝试改进common_model，看看启用多个滤波器是否会更稳健

  [测试]：对之前的多滤波器架构进行了思考，并实现，将位置与角度解耦，之后加入了一个耦合悉数，在实际的运动中，运动方向与朝向实际上有一定的关联，在当目标做正常前向运动时，速度方向应该与朝向大致一致，
  所以此时定义一个虚拟观测为h(x_trans, x_pose) = atan2(vy, vx) - θ，耦合强度的计算基于速度向量与朝向向量的一致性：
  * ρ = exp(-|v × e_θ|² / σ²)

  基于以上实现了对与运动的解耦，经过实际测试发现，确实有很大的改善，相较于不稳定的原始common_model有了很大的改善，但是似乎响应性有点过高，会因为位姿或者速度的突变而产生耦合的丢失一两次的跑飞预测
  决定保留改进的这个滤波器

  **梳理**：
    当前滤波器一共有四个，分别是常规运动的v3和偏向曲线运动的v4，以及小陀螺模型的rotating，以及基于v3改进解耦的四个滤波器，运动状态估计部分已经完成了80%，接下来需要合并梳理所有的运动模型，解决模型切换调用
    ，统一的在预测节点中管理，规整滤波器为统一的函数接口编写格式与命名，在预测节点中规整函数入口工厂，方便直接调用管理

2. 对滤波器模块进行规整，统一命名与接口
   采取编排器模式去重构现有代码，解构模块，留出函数接口，然后在节点中调用函数接口，串联起模块，简化当前流程、

   重构了解算节点的程序，采取编排器思想

# 7月22日

1. 补全延迟计算链路，视觉发出信号给到电控，电控收到信号控制电机到达对应的位置，为control延迟
   control信号收到，控制摩擦轮转动，到子弹飞出枪管为fire延迟


2. 串口通讯端
    | 帧头(2字节) | 数据(8字节) | 校验和(1字节) | 帧尾(1字节) |
    | 0xAA 0x55  | yaw(4) pitch(4) | checksum | 0xBB |

  考虑电机死区，对低于电机最小精度的转动角度都判断为无效数据，过滤掉


# 7月23日

1. 滤波器与运动模型的统一接口版本回退，统一接口导致当前预测端的数据稳定性大大下降，出现了大的偏差，回退解耦模型测试，恢复正常
    [待解决]：接口统一造成预测混乱

2. 上机测试：
    电控端[哨兵]：通讯接口为
    | 帧头(1字节) |          数据(4字节)                 |符号位（1字节） | 距离位（1字节）   |   导航3字节   | 控制模式（1字节）|  帧尾（1字节）|
    |   0xFF     | pitch低八位，高八位  yaw低八位，高八位 | yaw与pitch正负 | 千分位（补偿置0） |   当前置0     |   0x08        |    0xFE     |


    0x08控制模式字节为自瞄模式接口
    传递增量角度值，当前电控端的自瞄模式接口为左拨杆上置，开启小陀螺模式，开启摩擦轮拨弹盘进行自瞄模式判断接口   


# 7月24日
1. 修复了之前的相机坐标系弹道补偿，新增世界坐标系的弹道补偿
   
# 8月1日

1. 尝试解决陀螺仪的初始化问题，解决初始必须水平启动上电陀螺仪才会默认为当前为pitch等于0
    看了一下上位机，就是必须要求水平才能给到pitch初始化为0,需要重新思考一下怎么使用陀螺仪了

# 8月8日

1. 重构坐标变换，确保IMU的数据是实时传递到当前坐标变换中，确保一直在进行实时的坐标变换，杜绝将初始的角度当作0度去变换


# 8月14～16日

## 优化分类权重
搭建了一个完整的数据集，高质量样本每个种类训练集1000张，验证集与测试集200张，低质量样本每个种类训练集800张，验证集与测试集200张

8月14日：
	初版是在全低质量样本上训练的，对于哨兵的装甲板识别不是很好，在此基础上进行一套新的数据集训练 迭代训练，训练之后的新权重对于哨兵的识别有所改善，但是对于一些因为局部曝光，运动带来的模糊，装甲板贴纸破损等因素导致的识别分类出错，分类界限模糊，经过观察，对于迭代之后的第二版模型，对于分类出错的情况，大部分的识别的得到的置信度都在0.5以上0.8以下

	第三次测试：
		对于装甲板贴纸破损导致的识别分类模糊依然存在，对于部分遮挡依然可以识别，有较好的鲁棒性，尝试拉高置信度阈值测试，置信度拉高	以后会有改进，但是不是长久之计，过高的置信度会导致识别掉帧

8月15日：
	收集了一些困难样本，但是没训练的用第二版模型去测试，竟然全部都分类正确，看来泛化性比想象中更好，装甲板1测试通过，2测试不是很好，还是有必要训练的，困难样本数据不多，所以准备采用数据增强，一个是对局部曝光进行调整，来模拟实际场景下的曝光问题，一个是模拟局部遮挡

8月16日：
	数据集重新调整，对于high_quality每个种类训练集调整为800张，测试集于验证集调整为150张，low_quality每个种类训练集调整为600张，测试集与验证集调整为100张，困难样本则是每个种类调整为400张、
	训练完成，第三版模型对于一些部分遮挡的场景也能表现出较好的泛化性能，对于部分可能过于暗的数据可能表现的还不是很好

# 8月16日

1. 重新思考，重整了整个坐标系，首先是最为基础的，即固定世界坐标系，规定为水平垂直，固定的世界坐标系有助于后续的解算追踪，然后是PnP解算，得到相对相机的t_vec和r_vec，保持笛卡尔坐标系表示，将其变换到世界坐标系下，将其表示为目标向量，然后是IMU反馈的实时数据，将其表示为世界坐标系下的云台实时位姿数据，表示为云台朝向向量，然后控制电机转动的角度即两个向量对齐所需要转动的角度，这种数学抽象表示实际上基于SO(3)流形上的表示，是在三维旋转群下，两个向量都是SO(3)中的一个元素，自瞄的过程就是在这个流形上找到最短路径

# PS:注意点，即固定世界坐标系的选定，选定为机器人中心坐标系，至于怎么初始化还得思考一下，然后第二个就是两个向量在世界坐标下的表示以及求解对齐向量的需要转动的角度如何计算



# 8月17日

1. 对昨天的思路进行进一步思考，首先是坐标系的问题，没必要强硬的为固定世界坐标系确定唯一的三轴，只需要根据IMU的重力参考方向确定唯一的Z轴即可，至于另外两个的X与Y轴，就像重力为Z轴提供了绝对参考一样，没有绝对参考的情况下，X与Y轴也没办法确定，所以就保持这种任意性不管就行，至于如何计算角度，用到的算法是一轴两用，仅仅使用一个Z轴，以及云台姿态向量和目标位置 向量即可计算，以Z轴建立两个平面，一个是Z轴垂直的水平面，然后将两个向量投影下去，即可计算投影向量的夹角，得到需要转动的yaw角度，然后对于pitch角度，则是计算z轴归一化之后的投影分量长度，如果是0就是水平，如果是1,就是90度冲天，如果是-1，则是-90度对地，这样计算就能知道向量对齐需要转动的pitch角度
Z轴 ↑ (垂直向上)
            |
            |  / 目标方向向量（长度=1）
            | /
            |/ θ (这个角就是pitch)
    --------+--------> 水平面
           /|
          / |
         /  |
        /   ↓ z分量（垂直投影）

[测试]：不同姿态初始化，然后在回到固定位置，固定目标，测出来需要转动的yaw和pitch角度都是不变的，证明了当前思路应该是没有问题


# 现有解算模块流程：
假设检测到一个装甲板：

输入：像素坐标 [800, 600]
去畸变：校正镜头畸变 → [805, 598]
PnP求解：计算3D位置 → 相机坐标系 (500, -200, 1500) mm
坐标变换：

相机→IMU：应用标定外参
IMU→世界：应用重力对齐的旋转矩阵
结果：世界坐标系 (1200, 800, 300) mm


控制计算：

目标方向：归一化后的向量
云台朝向：从IMU姿态提取
使用一轴两用算法分解
输出：Δyaw = 15°, Δpitch = -5°


弹道补偿：

考虑重力和飞行时间
调整瞄准点
最终指令：Δyaw = 15.2°, Δpitch = -3.8°


# 8月18日

1. 灯条定位角点有时候会被背景光拉跑，暂时不知道什么原因，待解决


2. 
  # 滤波器预测优化

**现象观察**：
  1. 正常初始化以后，简单测试正常平移运动似乎还是出现预测滞后的情况
  2. 灯条定位掉角点很影响预测，运动情况不是很剧烈的情况也会因为角点被背景光拉走而导致预测点出现振动，快速运动的时候角点定位无法稳定，导致预测点会振荡更高
  3. 当快速移动的时候，如果目标部分超出视野了，预测点会直接乱飞
  4. 远处对于灯条的定位十分不稳定，猜测不稳定的检测无法提供稳定的连续的长时序的数据，导致预测不可信总是跑飞
  5. 观察发现当装甲板倾角大一些的时候，灯条检测似乎会更稳定一些

3. 
 # 灯条检测优化

1. 通过对ROI二值图的观察，ROI图像经过扩展以后，图像的尺寸总是剧烈变化，一会儿宽一会儿窄，不是一个好的现象，剧烈抖动带来的问题可能导致其中一个灯条飞出ROI区域
   然后就是装甲板侧对相机的时候，这个ROI区域图像较为稳定，两个灯条的二值化噪点少了很多，也比较稳定，但是正对着相机的时候，其中一个灯条就会有较大的噪点，抖动明显
   不稳定

2. 经过，改进了灯条定位检测，保证了稳定性，现在的预测跑飞的情况会大大降低，但是新问题出现，即YOLO定位检测不稳定，不够强大，远处小目标，快速运动目标都会导致掉检测



# 8月19日

1. 优化灯条检测的灰度化方法，新增了单通道提取，通道差分，HSV提取，与混合方法四种，目前测试单通道提取与通道差分，针对静止或者低速目标，两者都有较好的稳定性，但是通道差分方法应对运动的目标也能保持稳定的检测不掉角点，然后就是通道差分方法又出现了之前的老问题，即灯条内部过曝光导致阈值操作以后在二值化图中，灯条内部是黑色的，这是过曝的表现,测试了HSV
提取方法，与通道差分方法一样，阈值操作得到的二值化图像差不多，应对运动状态的目标也比单通道更强，但是会出现角点不稳定的情况，轻微抖动
  测试阈值方法，直方图的为原始的，本身效果就很好，替换了大律法，效果很差，现在怀疑ROI扩展有点小，扩大尝试为宽高均为原始ROI区域的1.5倍，经过测试，现在稳定性较高，但是新的问题，即YOLO检测定位装甲板的第一层神经网络出现了拖后腿的情况，常常是掉识别导致灯条检测也没办法运行


# 8月21日：
	1. 对之前的标签歧义进行优化，之前将红蓝装甲板全部放入images导致特征十分相似的红蓝装甲板，但是标签只有一个就会导致训练有阻碍，所以对数据集进行拆分，确保数据数据与标签对齐，然后采取了一些数据增强，包含模拟运动模糊，模拟低曝光，模拟遮挡

blue:
	08.21第一版：数据增强全部采用，未采用课程学习
	08.21第二版：去掉了数据增强，训练出来效果竟然比有数据增强的效果更好
	08.21第三版：炼一版v11版本的
red:
	08.21第一版：无数据增强，纯训练


# 8月22日

1. 训练总结：
  最初在思考整理数据集，最初数据集为红蓝装甲板为全部数据，但是标签只给其中一个种类，经过思考决定拆分数据集，解决这种数据噪声问题，训练了三版，分别是n模型的v8和v11，以及m模型的v8，经过对比发现，其全部指标均低于最初数据集m模型的v8训练，所以思考训练一版最初数据集的v11的m模型，经过测试发现，AP指标飙升，提升了4个点，但是随之而来的问题就是推理时间大大增加，由v8的7ms变为v11的42ms，推理速度换了精度，现在思考采用知识蒸馏的方法使用大模型的权重蒸馏传递给小模型n，保证高精度的同时兼顾推理速度

  [疑惑]：为什么全部装甲板的数据集竟然比单个种类的训练出来的数据更强，猜测是没有标签的对立装甲板充当背景起到了负样本的作用，同时刚好比例为1：1，强化了识别边界，多次测试发现这样操作下涨点3个点


# 8月22日：

1. 测试发现，解决标签歧义问题后训练的模型相比之前的第一版模型没有明显的进步，甚至部分情况下效果更差，检测的稳定性也不是很好，考虑换v11版本的尝试一下


# 8月23日：

1. 开始对整个框架的每个模块进行指标评估，有指标评估才能确定调参是否有效果
  [识别模块]：
    主要是接受相机线程——>识别定位装甲板——>取ROI一路分类，一路定位灯条角点——>排序过滤整理数据发布
    对整体的识别长期稳定性与定位角点的精度稳定性进行评估，一个是掉帧率，一个是角点像素抖动

经过观察，当前角点的定位像素还是较为稳定的，如果曝光与背景光都刚好的情况，基本不会掉识别，针对静止目标，常常抖动像素是上下一像素，比如660加减0.5左右，还是较为稳健的，然后是针对常规的平移目标，大多数情况灯条角点的像素坐标都呈现出一种正弦波，呈现出一种周期性，比如左右平移运动的时候，就是角点的x坐标在呈现大波峰的周期正弦波，此时角点的y坐标就是小波峰的正弦波，上下平移的时候则相反，前进后退的时候则是x与y坐标都在进行波峰峰值同频的正弦波，其他的一些常规的情况则是波形的一些叠加，这对于为滤波器调式带来一些思考


# 8月24日：

<!-- 1. 对各个阶段的数据进行平滑处理操作，首先是检测端，检测端本身就已经足够稳定，在常规平移的时候就已经表现出强大的稳定性，数据整体趋势都是较为平滑的正弦波，但是在静止的时候，通过对角点坐标的观察发现，定位之后精度单位给的太高，导致虽然整数位不变，但是小数位频繁波动，就会看起来是一个非常乱的数据，虽然波动范围不大，只有0.1像素左右，但是这种大波动带来的噪声还是会为后续的解算与估计引来坏的效果，所以尝试对精度进行截断，限制发布的数据为0.1精度，这时候观察静止的目标，角点坐标数据变为了阶梯数据，整数不变，然后0.1单位上，一直是0.3和0.4之间跳变，而且解算的数据也没有很大的改善，思考进一步优化滤波操作 -->

1. 观察发现，之前的检测端的角点的像素坐标是很稳定的，然后对比观察解算之后的世界坐标position和相机position的时候发现，数据波动最强烈的就是相机坐标下的z轴也就是世界坐标下的x轴，即目标到相机的距离，这时候沿着三轴运动的时候，另外两轴应对三轴运动的时候数据都表现较好的平滑，只有这个x轴应对沿着x轴前进的时候是稳态的，没有过大的数据波动的，但是应对沿另外两轴运动的时候就会出现较强的数据波动，然后就是滤波器的三轴预测，数据还算平滑，但是问题就出在延迟这块


# 8月25日：

1. 对预测器进行系统的分析与优化，首先是预测时间，时间断点为预测节点的观测时刻，观测时刻之前的为从图像采集，到推理分类再到解算的一段时间，观测时刻之后的时间分为下位机通讯时间，电机控制时间，以及子弹飞行时间，在不上实车的时候，只需要在图像采集的时候记录下时间戳，然后在预测节点中采集当前观测时间，做差即为前半段预测时间

2. 使用Plotjugger对话题数据进行订阅，通过数据曲线结合调试，在优化过程中主要发现三个主要的问题，预测滞后，预测超调以及剧烈波动，预测滞后与预测超调都是因为对于运动状态的适应性不够强大，在面对周期性的左右或者前后运动的时候，在速度不快的时候，主要表现为时间还算准确，但是预测量会超调，当运动速度变快的时候，此时不但预测量会超调，预测的时间也会滞后，整体表现为预测数据曲线想对解算数据曲线是相位滞后的，思考解决加大trans_q_acc，即加速度过程噪声，以此来提高Q/R比，加大对于运动状态快速响应的能力，调节之后有一定的改进，但是还是会有滞后的现象，暂时不知道如何解决，第三个问题是如何处理剧烈波动，当目标丢失，目标突然出现在视野，以及高速运动的时候，还有角点不稳定起飞的时候，都会出现剧烈振动，预测点直接起飞

3. 还是老问题，即像素的坐标的抖动，虽然精度很高，但是数据曲线呈现出的就是静止的时候，就是剧烈的波动，没有一个长期的稳定的平滑的数据，这回影响到后面的解算也是波动剧烈的数据，还有就是PnP相机z轴这个直接反应距离的解算数据，也是很不稳定，有剧烈的波动


# 8月26日：

1. 昨天尝试了对数据进行精度的量化，将尺寸限制在0.5精度，这样原本抖动的数据都收敛到0.5精度，虽然测试之后发现有所好转，但是依然会有数据的强烈的波动，而且当目标运动的时候，就失效了，所以查找资料以后决定尝试一欧元滤波器，对数据进行滤波，但是测试之后发现，静止的时候还凑合，但是一旦运动起来，就是一托，所以方案暂时放弃了

[失去所有手段]：
总结现有的问题：
  检测的角点坐标像素抖动带来逐层传播的误差，然后就是PnP的z轴解算不稳定，
  第三个就是滤波器参数调节困难，不知道到底是滤波器本身没有发挥到极致还是之前的误差和解算不稳定带来的问题


# 8月27日：

1. 换回了之前的常规运动模型，明显表现的比耦合运动模型表现的更加强大，更加稳定，同时对之前的问题进行了回顾
  
[数据抖动]：既然是静止的时候才能明显的看到数据抖动，运动的时候可以很好的进行正常的追踪，那么就暂时搁置针对数据抖动的修改，后续采用四点模型具体查看对比替换

<!-- [预测量超调与相位滞后以及目标突然出现在视野对滤波器预测的问题]：
换回了之前的整体滤波器，对于目标突然出现或者消失在视野中导致的运动状态突变，进而导致的数据剧烈抖动，整体滤波器可以实现很好的跟踪，不会出现剧烈超调或者预测点乱飞，
然后就是预测量超调和相位滞后，在测试过程中左右来回平移可能更看作是一种周期运动，对于这种周期运动，加速度和速度方向总是在有规律性的变化，而EKF在预测的时候往往马尔可夫过程
，即未来的预测只依赖当前状态，所以周期运动往往不会很好的快速响应，但是EKF也在尽力做预测进行跟随了，考虑使用第一人称视角的回放，对实际场上的目标真实的运动状态进行分析，同时
也能检验识别的稳定性 -->

# 已解决：参数全部0.1

[PnP的z轴解算不稳定性]：
暂时没有办法来解决，待思考

# 8月28日：

1. 详细的阅读了同济的开源，跑通了同济的yolov11，效果非常好，准备明天迁移
   另外有很多好的地方，准备后续迁移


# 8月29日：

1. 对昨天的检测数据进行收集，针对静止目标，角点坐标十分的稳定，竟然惊人的维持在一个固定的数值不动且长时间稳定，且精度也足够高，虽然过程中可能有一两秒是抖动了，但是可以接受

2. 四点(keypoints)模型调研：

都是要魔改，骨干网络，检测头，损失函数，标签编码等等，实际上想要端到端就要把多任务综合起来，目标检测，多标签分类，关键点keypoints检测等

[北科]：

实际上就是基于yolo_pose去修改的，其中在配置数据的时候给到kpt_shape[4,2],即关键点的数量和维度

# 北科采用的是关键点检测加上数字分类，也是双层神经网络检测，第一层是装甲板快速检测加上角点回归，第二层就是数字分类

13个标签数据
第0位: 装甲板的类别
第1-4位: 装甲板标注锚框,用于目标检测。四个数的顺序是x,y,w,h，其中x,y,是锚框中心点的归一化坐标，w,h 是归一化相对宽高。
第5-14位: 装甲板灯条的边界点，从前到后分别是:
左上角归一化坐标x1,y1
左下角归一化坐标x2,y2
右下角归一化坐标x3,y3
右上角归一化坐标x4,y4

[深大]：

# 深大采用的是8角点加上4颜色，再加上9分类，一个网络直出

在v5的基础上修改，backbone网络采用MobieNetV3

0到8是四个关键点，顺序从左上角开始逆时针；9到13是颜色（红蓝灰紫），13到22是数字

G（哨兵） 1（一号） 2（二号） 3（三号） 4（四号） 5（五号） O（前哨站） Bs（基地） Bb（基地大装甲）


[同济]：

# 同济采用的是多类别加关键点，多类别直接把数字分类，灯条颜色，装甲板大小全部包含

位置0-3：  边界框坐标 [x, y, w, h]              4个值
位置4-41： 38个装甲板类别的置信度分数           38个值  
位置42-49：4个关键点的坐标 [(x1,y1)...(x4,y4)]  8个值
总计：                                          50个值

  {blue, sentry, small},     {red, sentry, small},     {extinguish, sentry, small},
  {blue, one, small},        {red, one, small},        {extinguish, one, small},
  {blue, two, small},        {red, two, small},        {extinguish, two, small},
  {blue, three, small},      {red, three, small},      {extinguish, three, small},
  {blue, four, small},       {red, four, small},       {extinguish, four, small},
  {blue, five, small},       {red, five, small},       {extinguish, five, small},
  {blue, outpost, small},    {red, outpost, small},    {extinguish, outpost, small},
  {blue, base, big},         {red, base, big},         {extinguish, base, big},      {purple, base, big},       
  {blue, base, small},       {red, base, small},       {extinguish, base, small},    {purple, base, small},    
  {blue, three, big},        {red, three, big},        {extinguish, three, big}, 
  {blue, four, big},         {red, four, big},         {extinguish, four, big},  
  {blue, five, big},         {red, five, big},         {extinguish, five, big};


# 每个装甲板的完整标注格式
{
    "image": "image_001.jpg",
    "annotations": [
        {
            "class_id": 15,  # 38个类别中的一个（如：红色3号小装甲）
            "bbox": [320, 240, 80, 60],  # [中心x, 中心y, 宽, 高]
            "keypoints": [
                [290, 210],  # 左上角
                [350, 210],  # 右上角
                [350, 270],  # 右下角
                [290, 270]   # 左下角
            ]
        }
    ]
}


3. 又要开始重构了，太乱了....

[统一数据结构]：
灯条定位的pointer的lightbar和PonitPair
   

# 9月7日：

1. 数据结构重构完了，对同济的识别模块成功迁移，对静止目标，运动目标等多个数据进行观察，识别之后的角点数据还是比较准确的，静止目标的时候能够大多数时候都是恒定的，但是会有抖动，运动目标对数据曲线观察发现更加平滑，而且识别稳定性，准确性，以及泛化性都相对我们自研的模块都很大的进步
  但是对应也会有一定的缺点，就是整体识别对角点的定位可能精度相对传统识别会有一些衰减，比如倾斜目标的时候，对角点的定位是一个大概，因为四点模型还是基于YOLO修改的，所以在面对这种倾斜目标的时候输出的检测框与传统YOLO类似，是会扩大检测框来包含目标，所以在定位角点的时候，这种情况会保留，
  但是已经做到很好了，即使一个灯条消失也能精准的识别，后续会考虑对其v5和v8版本进行迁移，毕竟v11的推理耗时还是有点大

2. 接上之后的解算与预测模块发现，似乎对预测和解算并没有较大的提高，解算出来的三维坐标在静止的时候还是抖动的，运动目标的时候数据曲线也不是很平滑，而且似乎超调量更大了，费解


# 9月9日：

1. 昨天在测试的时候发现小目标检测能力很弱，最远处也就能看到2.4m左右，后来经过讨论发现，是镜头焦距太短，当前4mm不够用，一般都使用8mm或者6mm，另一个原因就是过大的原始图像会导致传给检测的时候原本就很小的目标被压缩，
  加大了小目标检测的难度，尝试加了一个ROI之后，效果显著提高，给了800*800的ROI之后，依旧是4mm，最远处稳定检测来到了3.8m左右，显著提高，所以ROI截取可以保留，既能加快检测速度，还能提高远处小目标检测的能力


2. ROI功能模块化：
  把ROI拆分出来模块化封装好，随拿随用
  封装完了，就两个核心函数，一个图像ROI，一个坐标变换


3. 回退之前的平移滤波器参数全为0.1的版本，还算能用，凑合用吧，至少不是延迟预测了，虽然可视化看起来很痴呆


# 再次重整文件结构：

1. 把滤波器提取出来还有motion_model，还有predict组成Target目标状态估计器，新增一个Tarcker用来当作追踪决策器专注于进行多目标状态的处理，Tracker里面要调用封装Target,
  然后针对性的选择进行估计，而不是全部估计，然后把弹道解算从anlesover中独立出来，作为aimer瞄准器，承接Tracker，之后承接同济的planer规划器，最后通过串口发出去



# 9月10日：

1. 上车测试，之前重构的坐标变换现在上车正常追踪了，但是问题很多

  1. 首先就是对持续运动的目标能够一直保持跟踪，但是如果是运动停下来或者突然运动都会慢半拍，时好时坏有时候很灵敏，有时候却跟不上
  2. pitch轴单独测试：向上运动的时候还是能正常跟踪的，但是向下运动的时候有时候能正常跟踪，但是有时候却出现半跟不跟点头的情况


# 9月11日：

[现象总结]：

1. 首先是长期静止放置目标，因为当前陀螺仪成本比较低，所以yaw轴会狂飘，之前对解算部分深度重构之后，上车测试之后表现较为良好，经过半小时测试，未移动的静止目标，云台也静止，此时解算得到的控制电机的detla_yaw和detla_pitch都是不会飘的，保持了长期稳定

2. 对于长期静止放置的目标，解算出发现这个detla_yaw和detla_pitch是会漂移的，忽高忽低，呈现一种正弦波的波动，但是数值偏差也不太大，就是0.1~0.3的漂移，怀疑是数值稳定性问题，与之前的识别抖动一样，暂时不知道怎么解决，直接精度截断不好搞，这样这个数据波形更乱了
但是对数据优化处理又不知道如何处理

3. 限制死区：或者说瞄准目标确定的区域，电控那边最开始是yaw轴正负2.5,pitch轴正负2,然后在前天测试，发现近处的，1m之内的目标会出现明明没有瞄准对齐，但是却不进行追踪，对detla_yaw和detla_pitch数据图对比发现，过于近的距离下，当前相机FOV角度过小，即使目标在视角边界，
此时计算出的detla角度因为在瞄准死区内，所以被强制归零了，所以才会不动，电控改了数据，将死区限制在0.5，对应电机的死区，修改之后发现近处能够正常追踪，但是同时视觉上位机要做限制，这边给到近处（3m内）死区1度，超过3m死区2度，解决了近处追踪锁定之后会短暂抖动，同时远处追踪更快

4. 近距离目标如果突然消失在视野中又快速的回到视野中，会出现突然的左右摆动
   近处左右摆动跟踪延迟，基本上不跟踪，整体detla_yaw变化呈现出正弦波的形式，数据波动在正负6之间，不知道为什么会出现近处不跟踪的问题
   目标迅速的靠近或者远离镜头的时候表现较为良好，即使过于近导致识别不到

   上面提到的近处回看录屏视频发现大多是50cm～80cm之间

   在大概1m的位置回看发现，基本的跟踪还是比较能够流畅运行的，虽然目标过快会飞出狭窄的FOV角度，导致跟踪不上但是起码视野内能够正常的跟踪，同时还有问题就是如果目标运动之后突然刹车，估计器这里是有点反应不过来的，有一点延迟，有1s或者半秒预测点还在框外，但是此时目标已经停止了，所以
   此时云台跟随，这一段反应不过来也会传递下去，表现就是跟着跟着，目标停止了，云台还向外抖动一段，但是会迅速的被停止的目标拉回来，这一段有云台抖动，视野内的目标突然出现的时候快速索敌还算较快，但是没有量化出来的数据


   得思考如何处理预测跑飞，一跑飞就会导致数据离谱，直接导致云台乱飞了

   整体观察下来，在50～80cm这种近距离中，表现不是很好，虽然也能保证一定得跟踪，但是受限于FOV视场角太小以及当前电控方案较为死板，导致近处跟随表现不好
   然后就是1.2～2.4m之间，这期间表现的都比较比错，但是也会有很多小问题，一个是运动急停的收敛抖动，一个是有时候容易预测跑飞进而导致云台乱飞，再有就是偷感十足，给人一种跟随半死不活，对应的是电控的响应较差，视觉上位机也有问题，一方面是简单的变化角度，相当于计算运动学只有位置变化，不给速度和加速度，
   导致没有合理的数据，只依赖下位机去控制，就会出现跟随不是很好，慢吞吞的现象


5. 需要思考对电控的程序进行理论推导一下，看看其控制上下限，以及控制的效率


# 9月12日：

[哨兵电控分析]：

# 核心其实就是三个数据，一个是yaw_add：需要转动的总的角度值，一个是*yaw：单次控制周期需要转动的角度，然后就是串级PID内环角度环计算的单次控制周期需要转动的角速度

1. 解析数据：
其中的0.000087是一个神秘的参数，似乎是一个比例参数，将总的角度拆分为单次转动角度

<!-- 
        if(find_target==1)
        {
            if(auto_shoot.yaw_add >=2.5-0.25||auto_shoot.yaw_add <=-2.5-0.25) // 这里是一个判断，如果增量大于正负2.5，就处理，因为死区给的2.0，然后处理转化还是干嘛，看不懂
            {
                *yaw = auto_shoot.yaw_add*-0.000087f;//
            }
            else if(auto_shoot.yaw_add==0)
            {
               
			   *yaw=0.0;        // 死区内传过来的数据就置0
               *pitch=0; 
            }
            else
            {
                *yaw = 0.0f;
                permiss_shoot_yaw=1;  // yaw轴已经瞄准了
            }
          } -->

2. 串级内环控制计算角速度：

基于当前的陀螺仪读数去计算角速度：

<!-- static float gimbal_PID_calc(gimbal_PID_t *pid, float get, float set, float error_delta)
{
    float err;
    if (pid == NULL)
    {
        return 0.0f;
    }
    pid->get = get;
    pid->set = set;

    err = set - get;
    pid->err = rad_format(err);
    pid->Pout = pid->kp * pid->err;
    pid->Iout += pid->ki * pid->err;
    pid->Dout = pid->kd * error_delta;
    abs_limit(&pid->Iout, pid->max_iout);
    pid->out = pid->Pout + pid->Iout + pid->Dout;
    abs_limit(&pid->out, pid->max_out);
    return pid->out;
} -->



# 9月18日：

[上机测试]：

[测试时间]：晚上7点～9点之间，走廊开灯，曝光4000,增益12
          最远处稳定识别是2m2，测试从1m到2m2之间逐个20cm增加，都能保证较好的命中率，但是都是在畸形的枪管坐标系下进行的

1. 昨天上车测试了之后发现很多问题
   首先就是枪管坐标系的转换，在参数中给到几乎3000mm才算是补偿成功，这跟硬补偿没区别，弹道解算也不知道是否起了作用

2. 今天白天重新测试，这时候曝光3000,增益不变，光圈最大的时候最远稳定识别3m左右

3. 对于近处至少是80cm以内的近处目标，8mm的镜头不是很好用，看的太窄，此外还会出现，过近的距离去瞄准的时候都会导致pitch轴上下晃动，最后直接抬到最高处，暂时不知道什么原因，此外还有就是过于近的时候，在图像边缘处的目标，虽然解算出来的需要转动的角度与4mm没啥区别但是就是不是很停话，也不会去瞄准纠正
   4mm的最远稳定识别就是在1.5m左右

[根因分析]：
1. 硬件原因：镜头的光圈，景深，焦距，AOV
            相机的FOV，曝光，增益，靶面尺寸（画幅）

          
2. 环境因素：不同时间不同光照条件下都会有所差别

3. 对于弹道解算，坐标变换，以及与下位机通信的控制部分需要排查问题，为什么不是很听话，为什么做不到很灵敏，以及如何消除抖动，何时算作瞄准了


[上机测试]：

1. 曝光4000，增益12

  今天测试2.4m~3m之间，几乎一直都是掉时别，对于静止和yaw轴扫描的情况都是这种情况，在特殊的角度会出现一直识别不到的情况
  3m开始测试，对比发现，上述情况保持，部分时候会出现识别保持较好的情况，扫描的时候还是会保持掉识别，3.5m以后掉识别的频率更高了
  4m以后几乎很难保证稳定识别，但是发现一个惊人的现象，即让目标保持在图像最下端，竟然还是能保持稳定识别，虽然是针对静止来说，对于运动扫描还是会掉识别
  但是相比之前已经有很大的好转，这个极限保持在5m左右，再远处，即使是保持在图像最下端也无法保持静止稳定识别了，也就是说，稳定的识别也就保持在2m出头，极限稳定的识别保持在5m
  左右


2. 现在提高曝光进行测试一下：
   曝光5000，增益12：

   效果一下好了很多，3m之前都能保证静止和扫描的时候稳定识别，3m开始出现掉识别，但是扫描和静止识别还算稳定，3.2m掉识别开始更加明显，此时扫描掉识别加剧，静止识别还算稳定
   但是也会出现掉时别明显的时候，但是经过后续逐个20cm的拉远，发现这个掉识别并不是一直明显，在3.2m到3.8m之间虽然也会出现掉识别但是大多数时间都是较为稳定的，在4m的时候情况开始出现变化，静止会出现部分情况一直不识别，但是当目标在图像下端的时候还是正常识别的
   4m以后开始就是随机性更强，在非下端的时候还是会出像静止识别稳定的时候，4.8m开始这个非下端静止稳定识别几乎就不存在了，只剩下下端的时候能够稳定识别了
   下端稳定识别的极限是6.5m

   曝光6000：
   对比5000曝光几乎没有什么变化，在曝光上已经对比结束了，再大即使有效果也没有意义了，单帧时间太长


# 9月19日：

[上机测试]：

下午1～3点之间，曝光4000，增益7,白天测试的时候受到走廊地面反光会很大，所以稳定打击的距离就在2m左右


[问题总结]：

1. 电控那边对于瞄准的判断有问题，测试了几次猜测电控的实际逻辑就是pitch先对齐，对齐之后就算瞄准了，多次测试发现都是yaw轴还有至少2度没有对齐，但是实际上云台已经不去对齐了，所以怀疑电控的瞄准判断逻辑
  在瞄准的时候，至少在2m以内，超过2度，现有的弹道就已经打不中了，超过2m以外，yaw的角度超过1度，就已经打不中了

2. 另外一个遗留问题就是pitch对齐的时候会出现去瞄准对齐的时候上下晃动，最后pitch直接飞天了，初步怀疑是pitch的FOV太短，导致去对齐的时候一旦没有真正瞄准到，在这个过程中，目标消失在视野，就被直接发0强制停止了

3. 然后就是角度跑飞的情况，yaw飞出了80，pitch间接性飞出70,十分费解，不知道是什么情况


# 9月20日：

[镜头FOV记录]：
（H*V）

1. 4mm镜头：70*50
   6mm镜头：63*44
   8mm镜头：45*32


[重构]:

识别传出来armors，包含：
std_msgs/Header header

# 新增字段
int32 target_id          # 目标ID（1英雄、3/4步兵、-1哨兵等）
int32 armor_count        # 检测到的装甲板数量
float32 roi_coefficient    # roi转换系数

# 装甲板信息
Armor[] armor

整个[pipeline]:
第1帧（初次检测）：
装甲板列表进入track函数 → 过滤出蓝色装甲板 → 排序选出最优的 → 状态是"lost" → 调用set_target → solver解算3D位姿（PnP+坐标变换+yaw优化） → 创建Target对象（初始化EKF） → found=true → 状态变为"detecting"
第2-3帧（建立跟踪）：
状态是"detecting" → 调用update_target → target.predict预测当前位置 → 找到匹配的装甲板 → solver解算新位姿 → target.update更新EKF → detect_count递增 → 状态变为"tracking"
第N帧（稳定跟踪）：
状态是"tracking" → predict预测 → 找到装甲板 → solve解算 → update融合 → 保持"tracking"状态
第N+1帧（临时遮挡）：
没有检测到3号装甲板 → update_target返回false → 状态变为"temp_lost" → 但仍然输出预测位置

1. 重构之后的节点就是检测之后直接进track节点，解算和预测估计器都不新开节点，而是作为模块去给track调用，所以现在需要先做封装，将solver和target进行封装，给到tracker去调用

坐标变换接口封装




# 9月22日：

1. pnp解算出来r_vec和t_vec，r_vec转R_camera2world，再变换到IMU，再变换到World

pnp_solver中的pnp_result中的R_armor_to_camera对应的是上面程序中的装甲板相对相机的旋转矩阵，现在我新设计了姿态结构体用来表示装甲板在世界坐标系下的姿态，将其加入到了Target结构体的定义中，现在的流程中只是把球坐标以及三维坐标将其转换到世界坐标系下了，接下来还需要将姿态结构体转换到世界坐标系下，然后做yaw优化，姿态结构体的填充需要从pnp解算得到的r_vec入手，r_vec转旋转矩阵以后，再转世界坐标系，yaw优化要用到重投影和迭代搜索，所以直接新开一个程序


现在是在解算的时候，为Target结构体加上了相机坐标系下的姿态和xyz三维坐标，所以需要在构造的时候把这两个也加上，以及解算的时候pitch和roll先验


# 9月24日：

1. solver模块需要初始化世界坐标系，这需要订阅IMU数据，然后设置相机内参，需要加载参数


# 9月26日：

1. 迁移重构完了解算，估计以及追踪这一托

获取了云台的yaw,然后规定搜索范围，之后传入当前的yaw，执行搜索，计算误差，传入了当前的yaw和坐标，然后传入坐标和yaw计算重投影到图像上的坐标，然后是重投影坐标的计算，进到重投影计算中，构建旋转矩阵，坐标转换，获取t_vec和r_vec，然后调用opencv函数，传入参数，传入的object_points是定义的装甲板坐标点，计算出对应的投影得到的image_points

pitch动是roll
yaw动是pitch
roll动是yaw

[相机坐标系]:
以镜头向外为z轴正向,向右为x轴正向,向下为y轴正向

[装甲板坐标系]:
X轴：向右

Y轴：向上

Z轴：向外（朝向观察者,与镜头方向z轴相对）

[INFO] [1759065876.703043162] [Tracker]: 状态转换: detecting -> tracking (检测次数: 3)
[2025-09-28 21:24:36.734] [debug] PnP求解成功
[2025-09-28 21:24:36.734] [debug] 旋转向量 - x:-0.026, y:-3.036, z:-0.280
[2025-09-28 21:24:36.739] [debug] PnP求解成功
[2025-09-28 21:24:36.739] [debug] 旋转向量 - x:-0.028, y:-3.047, z:-0.176



# 10月1日：

1. 成功迁移了同济的解算，估计与追踪联合的模块，但是遗留了超多的问题，因为是使用自带的IMU的进行初始化的，所以在构建坐标系变换的时候乱的一批
主要的问题就是解算那里有点崩，因为是集成的达秒IMU，所以重构了解算部分的各个数据结构，在坐标系的管理上出现了很大的问题，首先是在求解装甲板的自身姿态的时候，为了比较好理解，这里将装甲板的3D参考点坐标系定义的与相机坐标系一样，但是定义的坐标轴与默认意义上的ypr是不一样的，默认ypr对应的旋转轴分别是yaw围绕z轴旋转，pitch围绕y轴旋转，roll围绕x轴旋转，但是这里我重新定义了以后，它这个r_vec求解出来的姿态是围绕着对应相机坐标系三轴的ypr，映射下来就是
ypr——>ryp

所以后续在变换和优化yaw的时候都需要记住这个映射，第二个问题就是相机坐标系和世界坐标系的管理没有做好，将目标坐标从相机坐标系到世界坐标系进行变换还是比较好变换的，因为这时候不涉及装甲板自身坐标系的干预，所以直接调用标定的变换矩阵就秒了，但是一旦涉及到将姿态从装甲板自身变换到相机，再从相机变换到世界，这其中的旋转矩阵的相乘变换就很头疼，本来就变换很费解，再加上上面的装甲板轴系定义和ypr的颠倒，现在更不好变换了，这个变换不好解决，另外需要注意的是就是传给滤波器的ypda其中ypd是球坐标下的目标位置，然后a是装甲板自身倾角yaw，这几个角度数据都是弧度值

在原本的同济的开源中，在进行yaw优化的时候是从世界坐标系下进行重投影计算的，在迁移之后因为之前搞不明白旋转矩阵的连续变换，所以就在相机坐标系下进行重投影，现在需要优化

[需要优化]：
1. 坐标系之间的变换以及求解自身姿态和yaw优化这一套混乱的情况
2. 复用的函数要统一，更加精简结构，提高整洁和精简


修整了一些solver模块，去掉了一些没用的函数，将数据结构统一

2. 重新分析：

现在的对装甲板姿态求解的那部分是绕y轴旋转得到yaw角，绕x轴旋转得到pitch角度，绕z轴旋转得到roll

# 10月2日：

1. 

昨天对欧拉角提取函数加一个衍生版本，现在有两个版本，一个是yxz，还有一个基础的zyx，现在调用eulers_yxz得到的就是ypr，现在在此基础上进行旋转矩阵构建对应的是下面这个矩阵，现在solver中修整了一下，不会出现ypr映射出错的问题，就是记住现在是yxz分解即可

R = [cos(y)cos(r) + sin(y)sin(p)sin(r),  -cos(y)sin(r) + sin(y)sin(p)cos(r),   sin(y)cos(p) ]
    [cos(p)sin(r),                        cos(p)cos(r),                        -sin(p)      ]
    [-sin(y)cos(r) + cos(y)sin(p)sin(r),  sin(y)sin(r) + cos(y)sin(p)cos(r),   cos(y)cos(p) ]


# 10月3日：

1. 昨天在加新函数的功能的时候发现了问题，要想将世界坐标系的目标投影回图像上，就不能掺杂坐标系，
必须确保yaw和world_xyz都是属于世界坐标系，想要得到世界坐标系下的yaw就必须重新设计流程，重新定义装甲板3D点的轴方向，确保经过多轮变换之后得到的旋转矩阵提取以后得到的欧拉角就是装甲板自身的姿态，你需要确保这个连续变换的旋转矩阵的最后的轴系方向与世界坐标系的轴系方向相同

[变换过程]：

 R_imu_to_world_ = R_yaw_init_ * R_world_alignment_ * R_imu_current;
# 这里是一个IMU到世界坐标系的变换，其中yaw_init是对IMU初始数据到达的时候，将yaw偏移消除，置0构建的对齐矩阵之一，然后world_alignment是重力对齐矩阵，IMU_current则是实时的IMU数据反馈的变换矩阵

# 其中R_camera_to_imu则是标定的时候的四元数转的旋转矩阵，然后与上面的R_imu_to_world相乘得到相机坐标系下的目标转换到世界坐标系，其中camera_to_imu是固定的，
# yaw_init和world_alignment都是节点初始化时候接到第一帧IMU的数据的时候确定的，是固定的，只有IMU_current是动态的


R_camera_to_world = R_imu_to_world_ * R_camera_to_imu

R_armor_to_world = R_camera_to_world * R_armor_to_camera
                 =  R_imu_to_world_ * R_camera_to_imu *  R_armor_to_camera
<!-- 
# 想要装甲板转换到世界坐标系之后的旋转矩阵提取出来的欧拉角是自身姿态，就要确保这个旋转矩阵所对应的坐标系的三轴与世界坐标系三轴方向相同，要确保从旋转矩阵提取的欧拉角是姿态，需要注意两点，第一是3D点定义的时候要基于 -->

# 10月4日：

1. 之前一直都是思考如何从装甲板的3D点的设计能够去得到一个较为准确的变换之后的姿态，今天仔细测试和思考以后发现R_camera_to_world在相机不移动的情况下就是一个固定的矩阵，所以需要的就是得到一个R_armor_to_camera，
现在就思考假设一个标准的按照ZYX顺序提取欧拉角的正对相机的旋转矩阵，作为R_armor_to_world去反向尝试算出来此时的R_armor_to_camera应该是什么样？然后基于此去尝试计算出来此时的3D点应该是什么样的？基于此去设计一个较为标准的3D点


# 10月7日：

1. 经过几天的鏖战，经过验证，之前的倒推法也不好使，主要就是多次坐标系的变换导致变换之后的旋转矩阵很难提取出有效的欧拉角，在测试的时候发现，我设定装甲板本体坐标系和IMU坐标系三轴方向一样，因为相机变换到IMU是已知的，所以这里设定本体和IMU的坐标系一样，一个正对相机的装甲板，正常来说，pnp解算出来的r_vec转的旋转矩阵应该是与相机变换到IMU的旋转矩阵是转置的，但是实际解算出来发现pnp的不稳定会引入误差，这样的话，在坐标变换的过程中会将误差传递，但是旋转矩阵不会忽略误差带来的影响，如果这个时候你去提取欧拉角，就会很爆炸，一个yaw角度上的轻微晃动可能在转换之后并不是真正的数据，如果想要消除这个误差，就得将本体坐标系定义的与相机坐标系一样，这个时候优化的yaw就是比较准的，但是改变了本体坐标系，那么即使没有误差，后续提取欧拉角，你也不是直观的，而且会因为角度跳变等原因，会出现数据不平滑和不稳定，所以这条路就不暂时走不通了，后来发现可以是用向量法，就是对优化过的相机坐标系下的装甲板的法向量，然后转到世界坐标系下，去atan2计算yaw_world，这样计算出来的yaw_world趋势很好，且变化足够稳定平滑，但是也有问题，就是因为坐标系变化的几何原因，发现这个向量法提取的数值不准确，实际变化角度有20度，但是yaw_world只变化几度，原因是法向量中x和y分量在占比中过小，无法通过坐标变化将角度变化的趋势传递过去，所以就会导致数值响应不好


  armor.xyz_in_gimbal = R_camera2gimbal_ * xyz_in_camera + t_camera2gimbal_;

  armor.xyz_in_world = R_gimbal2world_ * armor.xyz_in_gimbal;
  
  xyz_in_world = R_gimbal2imubody_.transpose() * R_imubody2imuabs * R_gimbal2imubody_ * R_camera2gimbal_ * (R_camera2gimbal_ * xyz_in_camera + t_camera2gimbal_)


决定尝试同济的坐标变换方案，同济采用的是相机安装在枪管右侧，所以有t_camera2gimbal，同时云台坐标系为X向前，由枪管指向前方，Y轴向右，Z轴向下，然后是IMU坐标系，其中IMU安装在云台上面，但是方向与云台坐标系有所改动

IMU的X轴与云台的X轴相反，Y轴也相反，但是Z轴相同

然后是R_imubody2imuabs，这个是IMU传递过来的四元数

这是相机变换到云台坐标系：  armor.xyz_in_gimbal = R_camera2gimbal_ * xyz_in_camera + t_camera2gimbal_;
云台变换到世界坐标系：  armor.xyz_in_world = R_gimbal2world_ * armor.xyz_in_gimbal;


# 10月9日：


1. 已经成功迁移了同济的解算，估计与追踪的模块，成功实现正确的解算与完整的重投影并得到targets，通过对同济开源的阅读，接下来需要迁移的是aimer瞄准和shooter射击判断，以及在开源中提到的规划器，暂时没看到在src下的哪个程序调用了这个规划器，主要看standard和sentry,没太看懂这个规划器如何使用，但是先着手迁移aimer和shooter


# 10月10日

1. 对同济的自瞄再次深入阅读，了解到他们旧版本是与C板通讯，走CAN通讯，socketCan收发数据，新版本（加了规划器的）基本都走MicroUSB通讯了，收发数据更多了，然后引头文件引gimbal的基本都是新版本的，Src中基本带mpc的基本都是新版本的


# 10月11日： 

1. 迁移了gimbal，现在的接口是IMU数据回调订阅之后给到gimbal里面的四元数队列push接口，进行插值，之后把插值之后的四元数数据给到solver的updateIMU函数去初始化


# 10月18日：
1. 现场调试发现坐标系解算出现翻转，日志如下所示（节选）：
   ```
   [2025-10-18 16:18:18.748] [debug] 解算得到目标在云台坐标系下的为yaw:0.09, pitch为0.14, roll为0.04
   [2025-10-18 16:18:18.748] [debug] 解算得到目标在世界坐标系下的为yaw:-0.11, pitch为-0.07, roll为-3.09
   [2025-10-18 16:56:06.899] [debug] 得到的数据是w:0.15, x:0.99, y:-0.02, z:0.03
   ```
   最终定位为 `DmImu` 四元数解析顺序错误：固件包 0x04 输出的是 `W,X,Y,Z`，旧代码按照 `X,Y,Z,W` 解包，导致 `w≈0.99` 被错认成 `x`，云台→世界旋转矩阵额外翻转180°。
2. 修复：`src/io/device/imu/src/imu_driver.cpp` 中更新寄存器 0x04 的 `memcpy` 顺序，确保 `qw` 先读入，其后依次为 `qx/qy/qz`，并同步修正文档注释。
3. 验证：修改后在 `test_node` 中打印的四元数恢复为 `w≈±1`，世界坐标系与云台坐标系 yaw/pitch 保持同号，roll 不再接近 `±π`。


# 10月19日:

1. 自瞄终于搞定，着手修改电控端，编写计算力矩控制接收上位机的数据

## 经验记录：
[来自交的Readme]:
1. 即使接收 shoot_latency 也无法很好解决处理延迟问题，因为处理延迟包含相机图像传输、神经网络 detect、预测器预测、发送给电控的的延迟、电控到机械的延迟。可行的方案：统一采用 STM32 时间，采集图像时记录时刻和序号，一张图对应一个序号，对应一个预测，对应一个Shoot，对应一个发射指令，对应一个子弹飞出侦测。子弹飞出侦测记录的时间减去采集图像的时间就是延迟。

[问题回顾]：
<!-- 1. PnP距离解算精度下降，仅针对静止目标，在1.8m左右开始就有距离精度衰减，在2m的时候，误差达到了20cm，考虑做数据收集，拟合出一个衰减曲线，以解算得到的dis作为自变量，得到的曲线的拟合输出量尽量对应真实距离 -->
<!-- 2. 朝向角计算还是会出现大问题，受限于当前的YOLO粗框选，非真实角点会严重影响后续的朝向角计算，后续采取针对检测框的单独设计的计算依据也只针对大倾角的时候，检测框才会出现明显的拉伸变形，但是小倾角的时候，检测框还是趋近于正方形无法精准估计朝向角，待优化 -->
**已解决**： 加了灯条定位模块之后采用更加稳定的灯条角点来进行PnP解算和朝向角的解算

<!-- 3. 现阶段的平移运动模型滤波器在面对有朝向角的平移目标会出现倾斜瞬间预测滞后，飞出合理范围，原因可能是倾斜瞬间，解算的数据变化过大，超出了滤波器本身的适应性范围，无法及时的跟进处理 -->
**已解决**： 加入了朝向角的计算可以帮助更好的进行状态估计

<!-- 4. PnP的解算精度还是有限，当前解算的极限距离为3.2m，同时发现了一个隐藏的特性，即朝向角与距离的物理关系计算，PnP在解算的时候，解算的是目标的最远点到当前的相机的距离
   ，所以当目标有朝向角的时候，这个距离可以利用起来，因为装甲板的朝向角与自身装甲板半长有一个隐藏的物理关系，即Δr ≈ 67.5mm × sin(θ)  （装甲板半宽），这个Δr就是有朝向角
   时相对正对装甲板时候的距离的偏差，但是受限于当前朝向角计算与PnP解算都出现有累积误差，所以无法完成更加准确的精准计算 -->

<!-- 5. 当前滤波器即运动模型封装还是无法达到理想的效果，较大的问题就是噪声参数没有调节好，且对滤波器进行统一封装之后，发现封装的过于潦草，导致滤波器性能一直不好，噪声参数在迁移的时候全部乱了，需要针对性的重新调节滤波器参数，以及当前滤波器面对突然出现在视野中的目标预测点会乱飞，需要针对性的做一些处理 -->
<!-- **半解决**：换回了之前的v3,突然闯入的乱飞已解决，参数调节暂时可用，但是随之深入思考带来的另外三个问题待解决 -->


<!-- 6. 与电控端对齐的pitch出现了很大的问题，受限于陀螺仪的pitch约束，即必须在完全水平的情况才能确保pitch为0,所以如果初始上电的时候不是水平，就会导致解算就是不准确的，例如初始上电的时候是-8度，那么后续解算都是将这个-8度当作0度去解算，同时我们疑似发现后续解算的时候这个pitch是不会动态跟随解算的，所以解算得到的pitch是十分不值得信任的，需要大刀阔斧的修改 -->
**已解决**： 重构了解算模块

<!-- 7. 当前发给电控端的数据频率有点略低，且频率不会稳定的，而是随着时间递增频率衰减的单掉递增，且目前来看，发布频率为80多hz，还需要进一步提高，因为电控端的控制采样时间为1ms，控制hz为1000hz，需要进一步修改 -->
**已解决**：频率不需要过高，大概90hz就够用

8. 预测时间链路没有补全，电控端接收到控制信号，控制电机转动到对应位置的control时间，control之后转动摩擦轮发弹出枪管时间的延迟，然后就是子弹飞行时间延迟，还有值得思考的一点就是相机随着云台转动，这个过程中的control需要仔细思考一下，目标不动，云台追踪， 这时候会将追踪的时候云台的运动看成目标的运动，还有就是目标动，云台也动追，需要处理一下

<yaw>:
  左正右负
<pitch>
  上负下正

[待做清单]：
<!-- 1. 火控编写：在朝向角最小的时候，即解算最精准的时候才进行有效的打击 -->
<!-- 2. 反小陀螺运动模型编写 -->
3. 重启维护器，保证在灯条被打击瞬间，即闪烁的瞬间保留最后的数据，确保不会丢识别
4. 受限于无法为每个车辆的每个装甲板都分配唯一不变的ID，所以思考搭建


